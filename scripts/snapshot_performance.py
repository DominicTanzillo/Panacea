# Generated by Claude Code -- 2026-02-14
"""Create a timestamped performance snapshot for tracking model evolution.

Saves all key metrics, staleness experiment results, and statistical tests
to writeup/snapshots/ with date-time naming for longitudinal comparison.

Usage:
    python scripts/snapshot_performance.py
    python scripts/snapshot_performance.py --label "post-finetune-v2"
"""

import sys
import json
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime, timezone
from scipy import stats

ROOT = Path(__file__).parent.parent
RESULTS_DIR = ROOT / "results"
SNAPSHOT_DIR = ROOT / "writeup" / "snapshots"


def load_results():
    """Load all result files."""
    data = {}

    # Model comparison
    mc_path = RESULTS_DIR / "model_comparison.json"
    if mc_path.exists():
        with open(mc_path) as f:
            data["model_comparison"] = json.load(f)

    # Deep model results
    dm_path = RESULTS_DIR / "deep_model_results.json"
    if dm_path.exists():
        with open(dm_path) as f:
            data["deep_model"] = json.load(f)

    # Staleness experiment
    se_path = RESULTS_DIR / "staleness_experiment.json"
    if se_path.exists():
        with open(se_path) as f:
            data["staleness"] = json.load(f)

    return data


def compute_stat_tests(data: dict) -> dict:
    """Compute statistical tests comparing models."""
    tests = {}

    staleness = data.get("staleness", {})
    if not staleness:
        return tests

    cutoffs = staleness["cutoffs"]

    # Extract AUC-PR curves
    xgb_auc = [r["auc_pr"] for r in staleness["xgboost"]]
    pit_auc = [r["auc_pr"] for r in staleness["pitft"]]
    bas_auc = [r["auc_pr"] for r in staleness["baseline"]]

    # 1. Paired t-test: XGBoost vs PI-TFT AUC-PR across cutoffs
    t_stat, p_val = stats.ttest_rel(xgb_auc, pit_auc)
    tests["paired_ttest_xgb_vs_pitft"] = {
        "description": "Paired t-test on AUC-PR across staleness cutoffs",
        "t_statistic": round(float(t_stat), 4),
        "p_value": round(float(p_val), 6),
        "significant_at_005": p_val < 0.05,
        "interpretation": "XGBoost significantly better" if p_val < 0.05 and t_stat > 0 else "Not significant",
    }

    # 2. Wilcoxon signed-rank: XGBoost vs PI-TFT (non-parametric)
    try:
        w_stat, w_pval = stats.wilcoxon(xgb_auc, pit_auc)
        tests["wilcoxon_xgb_vs_pitft"] = {
            "description": "Wilcoxon signed-rank test on AUC-PR across cutoffs",
            "w_statistic": round(float(w_stat), 4),
            "p_value": round(float(w_pval), 6),
            "significant_at_005": w_pval < 0.05,
        }
    except ValueError:
        tests["wilcoxon_xgb_vs_pitft"] = {"error": "Identical values or too few samples"}

    # 3. Paired t-test: PI-TFT vs Baseline
    t_stat2, p_val2 = stats.ttest_rel(pit_auc, bas_auc)
    tests["paired_ttest_pitft_vs_baseline"] = {
        "description": "Paired t-test on AUC-PR: PI-TFT vs Baseline across cutoffs",
        "t_statistic": round(float(t_stat2), 4),
        "p_value": round(float(p_val2), 6),
        "significant_at_005": p_val2 < 0.05,
        "interpretation": "PI-TFT significantly better" if p_val2 < 0.05 and t_stat2 > 0 else "Not significant",
    }

    # 4. Linear regression on XGBoost degradation (slope of AUC-PR vs cutoff)
    slope_xgb, intercept_xgb, r_xgb, p_slope_xgb, se_xgb = stats.linregress(cutoffs, xgb_auc)
    tests["xgboost_degradation_rate"] = {
        "description": "Linear regression of XGBoost AUC-PR vs staleness cutoff",
        "slope_per_day": round(float(slope_xgb), 4),
        "intercept": round(float(intercept_xgb), 4),
        "r_squared": round(float(r_xgb ** 2), 4),
        "p_value": round(float(p_slope_xgb), 6),
        "interpretation": f"XGBoost loses ~{abs(slope_xgb):.3f} AUC-PR per day of staleness",
    }

    # 5. Linear regression on PI-TFT degradation
    slope_pit, intercept_pit, r_pit, p_slope_pit, se_pit = stats.linregress(cutoffs, pit_auc)
    tests["pitft_degradation_rate"] = {
        "description": "Linear regression of PI-TFT AUC-PR vs staleness cutoff",
        "slope_per_day": round(float(slope_pit), 4),
        "intercept": round(float(intercept_pit), 4),
        "r_squared": round(float(r_pit ** 2), 4),
        "p_value": round(float(p_slope_pit), 6),
        "interpretation": f"PI-TFT loses ~{abs(slope_pit):.3f} AUC-PR per day of staleness",
    }

    # 6. Effect size: Cohen's d for XGBoost vs PI-TFT
    diff = np.array(xgb_auc) - np.array(pit_auc)
    cohens_d = float(np.mean(diff) / np.std(diff, ddof=1)) if np.std(diff, ddof=1) > 0 else float("inf")
    tests["cohens_d_xgb_vs_pitft"] = {
        "description": "Cohen's d effect size for XGBoost vs PI-TFT AUC-PR",
        "cohens_d": round(cohens_d, 4),
        "interpretation": "large" if abs(cohens_d) > 0.8 else "medium" if abs(cohens_d) > 0.5 else "small",
    }

    # 7. At-peak comparison (2-day cutoff, best-case scenario)
    tests["peak_comparison"] = {
        "description": "Model performance at 2-day cutoff (freshest data)",
        "xgboost_auc_pr": round(xgb_auc[0], 4),
        "pitft_auc_pr": round(pit_auc[0], 4),
        "baseline_auc_pr": round(bas_auc[0], 4),
        "xgb_over_pitft_ratio": round(xgb_auc[0] / max(pit_auc[0], 1e-9), 2),
        "pitft_over_baseline_ratio": round(pit_auc[0] / max(bas_auc[0], 1e-9), 2),
    }

    return tests


def build_snapshot(data: dict, label: str = "") -> dict:
    """Build a complete performance snapshot."""
    now = datetime.now(timezone.utc)

    snapshot = {
        "timestamp": now.isoformat(),
        "date": now.strftime("%Y-%m-%d"),
        "time_utc": now.strftime("%H:%M:%S"),
        "label": label or f"snapshot-{now.strftime('%Y%m%d')}",
    }

    # Model performance summary
    mc = data.get("model_comparison", [])
    dm = data.get("deep_model", {})

    snapshot["models"] = {}

    for m in mc:
        name = m["model"]
        snapshot["models"][name] = {
            "auc_pr": round(m["auc_pr"], 6),
            "auc_roc": round(m["auc_roc"], 6),
            "f1": round(m["f1"], 6),
            "optimal_threshold": round(m.get("optimal_threshold", 0), 6),
            "mae_km": round(m.get("mae_km", 0), 2),
            "median_abs_error_km": round(m.get("median_abs_error_km", 0), 2),
        }

    if dm and "test" in dm:
        t = dm["test"]
        snapshot["models"]["PI-TFT"] = {
            "auc_pr": round(t["auc_pr"], 6),
            "auc_roc": round(t["auc_roc"], 6),
            "f1": round(t["f1"], 6),
            "optimal_threshold": round(t.get("optimal_threshold", 0), 6),
            "mae_km": round(t.get("mae_km", 0), 2),
            "median_abs_error_km": round(t.get("median_abs_error_km", 0), 2),
            "temperature": round(dm.get("temperature", 1.0), 6),
            "best_epoch": dm.get("best_epoch", 0),
            "recall_at_prec_30": round(t.get("recall_at_prec_30", 0), 4),
            "recall_at_prec_50": round(t.get("recall_at_prec_50", 0), 4),
            "recall_at_prec_70": round(t.get("recall_at_prec_70", 0), 4),
        }

    # Conformal prediction results
    if dm and "conformal" in dm:
        snapshot["conformal"] = dm["conformal"]

    # Staleness experiment: AUC-PR at each cutoff
    staleness = data.get("staleness", {})
    if staleness:
        snapshot["staleness_auc_pr"] = {
            "cutoffs_days": staleness["cutoffs"],
            "xgboost": [round(r["auc_pr"], 6) for r in staleness["xgboost"]],
            "pitft": [round(r["auc_pr"], 6) for r in staleness["pitft"]],
            "baseline": [round(r["auc_pr"], 6) for r in staleness["baseline"]],
        }
        # Also F1 at each cutoff
        snapshot["staleness_f1"] = {
            "cutoffs_days": staleness["cutoffs"],
            "xgboost": [round(r["f1"], 6) for r in staleness["xgboost"]],
            "pitft": [round(r["f1"], 6) for r in staleness["pitft"]],
            "baseline": [round(r["f1"], 6) for r in staleness["baseline"]],
        }
        # Recall at various precision thresholds
        snapshot["staleness_recall_at_prec50"] = {
            "cutoffs_days": staleness["cutoffs"],
            "xgboost": [round(r.get("recall_at_prec_50", 0), 4) for r in staleness["xgboost"]],
            "pitft": [round(r.get("recall_at_prec_50", 0), 4) for r in staleness["pitft"]],
        }

    # Statistical tests
    snapshot["statistical_tests"] = compute_stat_tests(data)

    # Dataset info
    snapshot["dataset"] = {
        "n_test_events": staleness.get("n_test_events", 2167),
        "n_positive": staleness.get("n_positive", 73),
        "positive_rate": round(73 / 2167, 6),
        "source": "ESA Kelvins Collision Avoidance Challenge",
    }

    return snapshot


def main():
    parser = argparse.ArgumentParser(description="Create performance snapshot")
    parser.add_argument("--label", type=str, default="", help="Optional label for this snapshot")
    args = parser.parse_args()

    print("Loading results ...")
    data = load_results()

    print("Building snapshot ...")
    snapshot = build_snapshot(data, label=args.label)

    # Save with timestamp
    SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = snapshot["date"].replace("-", "") + "_" + snapshot["time_utc"].replace(":", "")
    filename = f"performance_{timestamp}.json"
    if args.label:
        filename = f"performance_{timestamp}_{args.label}.json"

    out_path = SNAPSHOT_DIR / filename

    # Convert numpy/bool types for JSON serialization
    def convert(obj):
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, (np.bool_,)):
            return bool(obj)
        if isinstance(obj, bool):
            return obj
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

    with open(out_path, "w") as f:
        json.dump(snapshot, f, indent=2, default=convert)

    print(f"\nSnapshot saved: {out_path}")
    print(f"  Date: {snapshot['date']} {snapshot['time_utc']} UTC")
    print(f"  Label: {snapshot['label']}")

    # Print summary
    print(f"\n{'='*55}")
    print(f"  Model Performance Summary ({snapshot['date']})")
    print(f"{'='*55}")
    for name, metrics in snapshot["models"].items():
        print(f"  {name:40s} AUC-PR={metrics['auc_pr']:.4f}  F1={metrics['f1']:.4f}")

    if "statistical_tests" in snapshot:
        tests = snapshot["statistical_tests"]
        peak = tests.get("peak_comparison", {})
        if peak:
            print(f"\n  At 2-day cutoff:")
            print(f"    XGBoost / PI-TFT ratio: {peak.get('xgb_over_pitft_ratio', 0)}x")
            print(f"    PI-TFT / Baseline ratio: {peak.get('pitft_over_baseline_ratio', 0)}x")

        deg_xgb = tests.get("xgboost_degradation_rate", {})
        deg_pit = tests.get("pitft_degradation_rate", {})
        if deg_xgb:
            print(f"\n  Degradation rates:")
            print(f"    XGBoost: {deg_xgb.get('interpretation', '')}")
            print(f"    PI-TFT:  {deg_pit.get('interpretation', '')}")

        ttest = tests.get("paired_ttest_xgb_vs_pitft", {})
        if ttest:
            print(f"\n  Paired t-test (XGBoost vs PI-TFT): p={ttest.get('p_value', 0)}")

    print(f"{'='*55}")


if __name__ == "__main__":
    main()
