# Generated by Claude Code -- 2026-02-08
"""Download bulk CDM data from Space-Track.org.

Usage:
    # First run: discovers available fields, downloads recent high-Pc CDMs
    python scripts/download_spacetrack.py --identity you@email.com --password yourpass

    # Download specific date range (YYYY-MM-DD)
    python scripts/download_spacetrack.py --identity you@email.com --password yourpass \
        --start 2020-01-01 --end 2024-12-31

    # Store credentials in environment variables instead:
    set SPACETRACK_USER=you@email.com
    set SPACETRACK_PASS=yourpass
    python scripts/download_spacetrack.py

    # Quick test (100 CDMs only)
    python scripts/download_spacetrack.py --test

Requirements:
    pip install spacetrack requests
"""

import os
import sys
import json
import time
import argparse
import csv
from pathlib import Path
from datetime import datetime, timedelta

# Space-Track API base URL
BASE_URL = "https://www.space-track.org"
LOGIN_URL = f"{BASE_URL}/ajaxauth/login"
CDM_URL = f"{BASE_URL}/basicspacedata/query/class/cdm_public"

ROOT = Path(__file__).parent.parent
DATA_DIR = ROOT / "data" / "cdm_spacetrack"


def login_session(identity: str, password: str):
    """Create an authenticated requests session with Space-Track."""
    import requests

    session = requests.Session()
    resp = session.post(LOGIN_URL, data={
        "identity": identity,
        "password": password,
    })

    if resp.status_code != 200 or "failed" in resp.text.lower():
        print(f"Login failed (status {resp.status_code})")
        print(f"Response: {resp.text[:500]}")
        sys.exit(1)

    print("Authenticated with Space-Track.org")
    return session


def discover_fields(session) -> list[str]:
    """Discover available CDM_PUBLIC fields via the modeldef endpoint."""
    url = f"{BASE_URL}/basicspacedata/modeldef/class/cdm_public/format/json"
    resp = session.get(url)

    if resp.status_code != 200:
        print(f"Failed to get field definitions: {resp.status_code}")
        return []

    data = resp.json()

    # modeldef returns {"controller": ..., "data": [...]}
    items = data.get("data", data) if isinstance(data, dict) else data
    if not isinstance(items, list):
        print(f"Unexpected modeldef format: {type(data)}")
        print(json.dumps(data, indent=2)[:1000])
        return []

    fields = []
    print(f"\nCDM_PUBLIC fields ({len(items)} total):")
    for item in items:
        if isinstance(item, dict):
            name = item.get("Field", item.get("field", ""))
            ftype = item.get("Type", item.get("type", ""))
            fields.append(name)
            print(f"  {name:40s} {ftype}")

    return fields


def download_cdms_by_date_range(
    session,
    start_date: str,
    end_date: str,
    output_dir: Path,
    batch_days: int = 30,
    min_pc: float = 0.0,
):
    """
    Download CDMs in date-range batches to respect rate limits.

    Args:
        session: authenticated requests session
        start_date: YYYY-MM-DD start
        end_date: YYYY-MM-DD end
        output_dir: directory to save CSV files
        batch_days: days per API request (30 = ~1 month chunks)
        min_pc: minimum collision probability filter (0 = all CDMs)
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    total_cdms = 0
    batch_num = 0
    all_rows = []
    fieldnames = None

    current = start
    while current < end:
        batch_end = min(current + timedelta(days=batch_days), end)
        date_range = f"{current.strftime('%Y-%m-%d')}--{batch_end.strftime('%Y-%m-%d')}"
        batch_num += 1

        # Build query URL
        url = f"{CDM_URL}/CREATION_DATE/{date_range}/orderby/TCA asc/format/json"
        if min_pc > 0:
            url = f"{CDM_URL}/CREATION_DATE/{date_range}/PC/%3E{min_pc}/orderby/TCA asc/format/json"

        print(f"\nBatch {batch_num}: {date_range} ...", end=" ", flush=True)

        # Rate limiting: max 30 requests/minute
        time.sleep(2.5)

        try:
            resp = session.get(url, timeout=120)
        except Exception as e:
            print(f"ERROR: {e}")
            current = batch_end
            continue

        if resp.status_code == 429:
            print("RATE LIMITED — waiting 60s ...")
            time.sleep(60)
            continue  # retry same batch

        if resp.status_code != 200:
            print(f"HTTP {resp.status_code}")
            current = batch_end
            continue

        try:
            data = resp.json()
        except json.JSONDecodeError:
            print(f"Invalid JSON response ({len(resp.text)} bytes)")
            current = batch_end
            continue

        if not data:
            print("0 CDMs")
            current = batch_end
            continue

        n_cdms = len(data)
        total_cdms += n_cdms
        print(f"{n_cdms} CDMs (total: {total_cdms})")

        # Track field names from first batch
        if fieldnames is None:
            fieldnames = list(data[0].keys())

        all_rows.extend(data)

        # Save intermediate checkpoint every 10 batches
        if batch_num % 10 == 0:
            _save_checkpoint(all_rows, fieldnames, output_dir, batch_num)

        current = batch_end

    # Final save
    if all_rows and fieldnames:
        output_file = output_dir / "cdm_spacetrack_all.csv"
        _save_csv(all_rows, fieldnames, output_file)
        print(f"\nSaved {total_cdms} CDMs to {output_file}")
    else:
        print("\nNo CDMs downloaded.")

    return total_cdms


def download_high_risk_cdms(session, output_dir: Path):
    """
    Download CDMs with elevated collision probability (Pc > 1e-5).
    These are the most valuable for training — the positive examples we need.
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    print("\n=== Downloading HIGH-RISK CDMs (Pc > 1e-5) ===")

    # Query for high-probability conjunctions across all time
    # Space-Track may limit results, so we batch by year
    all_rows = []
    fieldnames = None

    for year in range(2018, 2027):
        date_range = f"{year}-01-01--{year}-12-31"
        url = (f"{CDM_URL}/CREATION_DATE/{date_range}"
               f"/PC/%3E1e-5/orderby/TCA asc/format/json/limit/100000")

        print(f"  {year}: ", end="", flush=True)
        time.sleep(3)

        try:
            resp = session.get(url, timeout=180)
        except Exception as e:
            print(f"ERROR: {e}")
            continue

        if resp.status_code == 429:
            print("RATE LIMITED — waiting 60s")
            time.sleep(60)
            continue

        if resp.status_code != 200:
            print(f"HTTP {resp.status_code}")
            continue

        try:
            data = resp.json()
        except json.JSONDecodeError:
            print(f"Invalid JSON")
            continue

        if not data:
            print("0 CDMs")
            continue

        if fieldnames is None:
            fieldnames = list(data[0].keys())

        all_rows.extend(data)
        print(f"{len(data)} CDMs")

    if all_rows and fieldnames:
        output_file = output_dir / "cdm_high_risk.csv"
        _save_csv(all_rows, fieldnames, output_file)
        print(f"\nSaved {len(all_rows)} high-risk CDMs to {output_file}")
    else:
        print("\nNo high-risk CDMs found.")

    return len(all_rows)


def download_all_cdms(session, output_dir: Path, start_year: int = 2020, end_year: int = 2026):
    """
    Download ALL CDMs year by year in monthly batches.
    This is the big pull — may take 30-60 minutes due to rate limits.
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\n=== Downloading ALL CDMs ({start_year}-{end_year}) ===")

    total = download_cdms_by_date_range(
        session,
        start_date=f"{start_year}-01-01",
        end_date=f"{end_year}-12-31",
        output_dir=output_dir,
        batch_days=30,
    )
    return total


def _save_csv(rows: list[dict], fieldnames: list[str], path: Path):
    """Save list of dicts to CSV."""
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        writer.writeheader()
        writer.writerows(rows)


def _save_checkpoint(rows: list[dict], fieldnames: list[str], output_dir: Path, batch_num: int):
    """Save intermediate checkpoint."""
    ckpt = output_dir / f"checkpoint_batch{batch_num}.csv"
    _save_csv(rows, fieldnames, ckpt)
    print(f"  [Checkpoint saved: {len(rows)} CDMs → {ckpt.name}]")


def main():
    parser = argparse.ArgumentParser(description="Download CDMs from Space-Track.org")
    parser.add_argument("--identity", type=str, default=os.environ.get("SPACETRACK_USER", ""),
                        help="Space-Track username/email (or set SPACETRACK_USER env var)")
    parser.add_argument("--password", type=str, default=os.environ.get("SPACETRACK_PASS", ""),
                        help="Space-Track password (or set SPACETRACK_PASS env var)")
    parser.add_argument("--start", type=str, default="2020-01-01",
                        help="Start date YYYY-MM-DD (default: 2020-01-01)")
    parser.add_argument("--end", type=str, default="2026-02-08",
                        help="End date YYYY-MM-DD (default: today)")
    parser.add_argument("--high-risk-only", action="store_true",
                        help="Only download high-risk CDMs (Pc > 1e-5)")
    parser.add_argument("--discover", action="store_true",
                        help="Just discover available fields and exit")
    parser.add_argument("--test", action="store_true",
                        help="Quick test: download 100 CDMs and exit")
    args = parser.parse_args()

    if not args.identity or not args.password:
        print("ERROR: Space-Track credentials required.")
        print("  Option 1: --identity you@email.com --password yourpass")
        print("  Option 2: set SPACETRACK_USER and SPACETRACK_PASS environment variables")
        sys.exit(1)

    session = login_session(args.identity, args.password)

    if args.discover:
        discover_fields(session)
        return

    if args.test:
        print("\n=== TEST MODE: Downloading 100 CDMs ===")
        url = f"{CDM_URL}/orderby/TCA desc/limit/100/format/json"
        resp = session.get(url)
        if resp.status_code == 200:
            data = resp.json()
            print(f"Got {len(data)} CDMs")
            if data:
                print(f"\nFields: {list(data[0].keys())}")
                print(f"\nSample CDM:")
                for k, v in data[0].items():
                    print(f"  {k}: {v}")

                # Save test sample
                DATA_DIR.mkdir(parents=True, exist_ok=True)
                test_file = DATA_DIR / "test_sample.json"
                with open(test_file, "w") as f:
                    json.dump(data, f, indent=2)
                print(f"\nSaved to {test_file}")
        else:
            print(f"Failed: HTTP {resp.status_code}")
        return

    # Main download flow
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    if args.high_risk_only:
        n = download_high_risk_cdms(session, DATA_DIR)
    else:
        # Always download high-risk first (most valuable)
        n_hr = download_high_risk_cdms(session, DATA_DIR)

        # Then download everything
        n_all = download_all_cdms(session, DATA_DIR,
                                   start_year=int(args.start[:4]),
                                   end_year=int(args.end[:4]))

        n = n_hr + n_all

    print(f"\n{'='*60}")
    print(f"  Download complete: {n} total CDMs")
    print(f"  Saved to: {DATA_DIR}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
