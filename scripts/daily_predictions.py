# Generated by Claude Code -- 2026-02-13
"""Daily conjunction prediction and maneuver detection pipeline.

Designed to run as a GitHub Actions cron job (daily at 00:00 UTC).
Can also be run locally for testing.

Flow:
  1. Fetch latest TLEs from CelesTrak (free, no auth)
  2. Run pairwise orbital screening (altitude + RAAN filter)
  3. Score top candidates with baseline model
  4. Compare with yesterday's TLEs to detect maneuvers
  5. Validate yesterday's predictions against maneuver detections
  6. Log everything to Firebase + local JSONL

Usage:
    python scripts/daily_predictions.py              # Full daily run
    python scripts/daily_predictions.py --quick      # Test with 1000 sats
    python scripts/daily_predictions.py --validate-only  # Only check yesterday
"""

import sys
import json
import math
import argparse
import numpy as np
import requests
from pathlib import Path
from datetime import datetime, timedelta

ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT))

from src.data.maneuver_detector import (
    detect_maneuvers,
    extract_orbital_elements,
    load_tle_snapshot,
    save_tle_snapshot,
    mean_motion_to_sma,
    EARTH_RADIUS_KM,
    STARLINK_DELTA_A_THRESHOLD_M,
    DEFAULT_DELTA_A_THRESHOLD_M,
)
from src.data.firebase_client import PredictionLogger

CELESTRAK_URL = "https://celestrak.org/NORAD/elements/gp.php"
SNAPSHOT_DIR = ROOT / "data" / "tle_snapshots"
LOG_DIR = ROOT / "data" / "prediction_logs"


def fetch_active_tles(max_objects: int = None) -> list[dict]:
    """Fetch active satellite TLEs from CelesTrak (free, no auth)."""
    print("Fetching active satellite TLEs from CelesTrak ...")
    resp = requests.get(f"{CELESTRAK_URL}?GROUP=active&FORMAT=json", timeout=60)
    resp.raise_for_status()
    tles = resp.json()
    print(f"  Got {len(tles)} active satellites")

    if max_objects and len(tles) > max_objects:
        tles = tles[:max_objects]
        print(f"  Truncated to {max_objects} for quick mode")

    return tles


def fetch_starlink_tles() -> list[dict]:
    """Fetch Starlink TLEs separately (largest constellation)."""
    print("Fetching Starlink TLEs ...")
    resp = requests.get(f"{CELESTRAK_URL}?GROUP=starlink&FORMAT=json", timeout=60)
    resp.raise_for_status()
    tles = resp.json()
    print(f"  Got {len(tles)} Starlink satellites")
    return tles


def screen_pairs(tles: list[dict], alt_band_km: float = 50.0, raan_band_deg: float = 30.0) -> list[dict]:
    """Fast pairwise orbital screening using altitude + RAAN proximity.

    Filters ~99% of pairs, keeping only those in similar orbital shells.
    """
    n = len(tles)
    print(f"\nScreening {n} satellites for close approaches ...")

    # Extract orbital elements
    altitudes = np.zeros(n)
    raans = np.zeros(n)
    norad_ids = []
    names = []

    for i, tle in enumerate(tles):
        mm = float(tle.get("MEAN_MOTION", 0))
        sma = mean_motion_to_sma(mm)
        altitudes[i] = sma - EARTH_RADIUS_KM
        raans[i] = float(tle.get("RA_OF_ASC_NODE", 0))
        norad_ids.append(int(tle.get("NORAD_CAT_ID", 0)))
        names.append(tle.get("OBJECT_NAME", "UNKNOWN"))

    # Vectorized filtering: altitude overlap
    alt_diff = np.abs(altitudes[:, None] - altitudes[None, :])
    alt_mask = alt_diff < alt_band_km

    # RAAN proximity (handle wrap-around at 360°)
    raan_diff = np.abs(raans[:, None] - raans[None, :])
    raan_diff = np.minimum(raan_diff, 360 - raan_diff)
    raan_mask = raan_diff < raan_band_deg

    # Combine: both conditions AND upper triangle only (avoid duplicates)
    combined = alt_mask & raan_mask & np.triu(np.ones((n, n), dtype=bool), k=1)
    pairs_i, pairs_j = np.where(combined)

    print(f"  Found {len(pairs_i)} candidate pairs (from {n*(n-1)//2} total)")

    # Also extract inclination for better scoring
    inclinations = np.zeros(n)
    eccentricities = np.zeros(n)
    for i, tle in enumerate(tles):
        inclinations[i] = float(tle.get("INCLINATION", 0))
        eccentricities[i] = float(tle.get("ECCENTRICITY", 0))

    # Compute inclination differences for candidate pairs
    inc_diff = np.abs(inclinations[:, None] - inclinations[None, :])

    # Score candidates: combine altitude gap, RAAN proximity, and inclination
    candidates = []
    for idx in range(len(pairs_i)):
        i, j = int(pairs_i[idx]), int(pairs_j[idx])
        alt_gap = abs(altitudes[i] - altitudes[j])
        avg_alt = (altitudes[i] + altitudes[j]) / 2
        rd = float(raan_diff[i, j])
        id_ = float(inc_diff[i, j])

        # Composite risk heuristic:
        #   - altitude closeness (0-1): closer = higher risk
        #   - RAAN proximity (0-1): same plane = higher risk
        #   - inclination similarity (0-1): coplanar = higher risk
        alt_score = max(0, 1.0 - alt_gap / alt_band_km)
        raan_score = max(0, 1.0 - rd / raan_band_deg)
        inc_score = max(0, 1.0 - id_ / 10.0)  # 10° inc diff -> 0 score

        # Weighted combination: RAAN matters most for close approaches
        risk_score = 0.2 * alt_score + 0.5 * raan_score + 0.3 * inc_score

        candidates.append({
            "sat1_norad": norad_ids[i],
            "sat2_norad": norad_ids[j],
            "sat1_name": names[i],
            "sat2_name": names[j],
            "altitude_km": round(avg_alt, 1),
            "alt_gap_km": round(alt_gap, 2),
            "raan_diff_deg": round(rd, 2),
            "inc_diff_deg": round(id_, 2),
            "risk_score": round(risk_score, 4),
        })

    # Sort by risk score descending
    candidates.sort(key=lambda c: c["risk_score"], reverse=True)
    return candidates


def load_baseline_model():
    """Load the orbital shell baseline model if available."""
    model_path = ROOT / "models" / "baseline.json"
    if not model_path.exists():
        print("  Baseline model not found — using heuristic scoring")
        return None

    try:
        from src.model.baseline import OrbitalShellBaseline
        model = OrbitalShellBaseline.load(model_path)
        print(f"  Loaded baseline model from {model_path}")
        return model
    except Exception as e:
        print(f"  Failed to load baseline model: {e}")
        return None


def score_with_model(candidates: list[dict], model) -> list[dict]:
    """Re-score candidates using the trained baseline model."""
    if model is None or not candidates:
        return candidates

    altitudes = np.array([c["altitude_km"] for c in candidates])
    risk_probs, miss_estimates = model.predict(altitudes)

    for i, cand in enumerate(candidates):
        cand["model_risk_score"] = round(float(risk_probs[i]), 6)
        cand["model_miss_km"] = round(float(np.expm1(miss_estimates[i])), 1)
        cand["model_used"] = "OrbitalShellBaseline"

    # Re-sort by model risk score
    candidates.sort(key=lambda c: c.get("model_risk_score", 0), reverse=True)
    return candidates


def validate_yesterday(
    logger: PredictionLogger,
    today_tles: list[dict],
    yesterday_str: str,
) -> dict:
    """Validate yesterday's predictions against today's TLE data.

    Checks if any predicted high-risk satellites actually maneuvered.
    """
    print(f"\nValidating predictions from {yesterday_str} ...")

    # Load yesterday's TLE snapshot
    yesterday_snapshot = SNAPSHOT_DIR / f"{yesterday_str}.json"
    if not yesterday_snapshot.exists():
        print(f"  No TLE snapshot for {yesterday_str} — skipping validation")
        return {"validated": False, "reason": "no_snapshot"}

    prev_tles = load_tle_snapshot(yesterday_snapshot)

    # Detect maneuvers
    maneuvers = detect_maneuvers(
        prev_tles, today_tles,
        threshold_m=DEFAULT_DELTA_A_THRESHOLD_M,
    )
    print(f"  Detected {len(maneuvers)} maneuvers since yesterday")

    # Build maneuver lookup
    maneuvered_ids = {m["norad_id"] for m in maneuvers}
    maneuver_by_id = {m["norad_id"]: m for m in maneuvers}

    # Load yesterday's predictions
    predictions = logger.get_predictions_for_date(yesterday_str)
    if not predictions:
        print(f"  No predictions found for {yesterday_str}")
        return {
            "validated": True,
            "n_maneuvers": len(maneuvers),
            "n_predictions": 0,
            "top_maneuvers": maneuvers[:10],
        }

    # Check predictions against maneuvers
    outcomes = []
    correct = 0
    total_checked = 0

    for pred in predictions:
        sat1 = pred.get("sat1_norad", 0)
        sat2 = pred.get("sat2_norad", 0)
        risk = pred.get("model_risk_score", pred.get("risk_score", 0))

        sat1_maneuvered = sat1 in maneuvered_ids
        sat2_maneuvered = sat2 in maneuvered_ids
        either_maneuvered = sat1_maneuvered or sat2_maneuvered

        outcome = {
            "sat1_norad": sat1,
            "sat2_norad": sat2,
            "sat1_name": pred.get("sat1_name", ""),
            "sat2_name": pred.get("sat2_name", ""),
            "predicted_risk": risk,
            "sat1_maneuvered": sat1_maneuvered,
            "sat2_maneuvered": sat2_maneuvered,
            "either_maneuvered": either_maneuvered,
        }

        if sat1_maneuvered and sat1 in maneuver_by_id:
            m = maneuver_by_id[sat1]
            outcome["sat1_delta_a_m"] = m["delta_a_m"]
            outcome["sat1_delta_v_m_s"] = m["delta_v_m_s"]

        if sat2_maneuvered and sat2 in maneuver_by_id:
            m = maneuver_by_id[sat2]
            outcome["sat2_delta_a_m"] = m["delta_a_m"]
            outcome["sat2_delta_v_m_s"] = m["delta_v_m_s"]

        outcomes.append(outcome)

        # For accuracy: did we predict risk > 0.5 and a maneuver happened?
        if risk > 0.3:
            total_checked += 1
            if either_maneuvered:
                correct += 1

    # Log outcomes
    if outcomes:
        logger.log_outcomes(yesterday_str, outcomes)

    accuracy = correct / max(total_checked, 1)
    summary = {
        "validated": True,
        "prediction_date": yesterday_str,
        "n_maneuvers_total": len(maneuvers),
        "n_predictions": len(predictions),
        "n_high_risk_predictions": total_checked,
        "n_correct": correct,
        "accuracy": round(accuracy, 4),
        "top_maneuvers": maneuvers[:10],
    }

    print(f"  Predictions: {len(predictions)}, High-risk checked: {total_checked}, "
          f"Correct: {correct}, Accuracy: {accuracy:.1%}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="Daily conjunction prediction pipeline")
    parser.add_argument("--quick", action="store_true", help="Quick test with fewer satellites")
    parser.add_argument("--validate-only", action="store_true", help="Only validate yesterday")
    parser.add_argument("--top-k", type=int, default=100, help="Number of top pairs to log")
    parser.add_argument("--no-firebase", action="store_true", help="Skip Firebase, local only")
    args = parser.parse_args()

    today = datetime.now(tz=None)  # UTC-naive for compatibility
    today_str = today.strftime("%Y-%m-%d")
    yesterday_str = (today - timedelta(days=1)).strftime("%Y-%m-%d")

    print(f"{'='*60}")
    print(f"  Panacea Daily Prediction Pipeline")
    print(f"  Date: {today_str}")
    print(f"{'='*60}\n")

    # Initialize logger
    if args.no_firebase:
        # Force local-only by not setting env var
        import os
        os.environ.pop("FIREBASE_SERVICE_ACCOUNT", None)

    logger = PredictionLogger(local_dir=LOG_DIR)

    # Fetch today's TLEs
    max_objects = 1000 if args.quick else None
    active_tles = fetch_active_tles(max_objects=max_objects)

    # Save today's TLE snapshot for tomorrow's comparison
    SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)
    snapshot_path = SNAPSHOT_DIR / f"{today_str}.json"
    save_tle_snapshot(active_tles, snapshot_path)
    print(f"  Saved TLE snapshot to {snapshot_path}")

    # Validate yesterday's predictions
    validation = validate_yesterday(logger, active_tles, yesterday_str)

    if args.validate_only:
        print("\n  Validation-only mode — done.")
        return

    # Screen pairs
    candidates = screen_pairs(active_tles)

    if not candidates:
        print("\n  No candidate pairs found. Done.")
        return

    # Score with baseline model
    model = load_baseline_model()
    candidates = score_with_model(candidates, model)

    # Take top-K predictions
    top_k = min(args.top_k, len(candidates))
    top_predictions = candidates[:top_k]

    print(f"\n  Top {top_k} highest-risk pairs:")
    for i, pred in enumerate(top_predictions[:10]):
        risk = pred.get("model_risk_score", pred.get("risk_score", 0))
        print(f"    {i+1:3d}. {pred['sat1_name']:20s} <-> {pred['sat2_name']:20s} | "
              f"risk={risk:.4f} | alt={pred['altitude_km']:.0f}km")

    # Log predictions
    print(f"\nLogging {top_k} predictions ...")
    logger.log_predictions(today_str, top_predictions)

    # Daily summary
    summary = {
        "date": today_str,
        "n_satellites_screened": len(active_tles),
        "n_candidate_pairs": len(candidates),
        "n_predictions_logged": top_k,
        "top_risk_score": top_predictions[0].get("model_risk_score",
                                                   top_predictions[0].get("risk_score", 0)),
        "validation": validation,
    }
    logger.log_daily_summary(today_str, summary)

    # Archive to HuggingFace (weekly)
    if today.weekday() == 6:  # Sunday
        print("\n  Weekly archival to HuggingFace ...")
        try:
            archive_to_huggingface(logger)
        except Exception as e:
            print(f"  HuggingFace archival failed: {e}")

    print(f"\n{'='*60}")
    print(f"  Daily pipeline complete!")
    print(f"  Pairs screened: {len(candidates)}")
    print(f"  Predictions logged: {top_k}")
    if validation.get("validated"):
        print(f"  Yesterday's accuracy: {validation.get('accuracy', 0):.1%}")
    print(f"{'='*60}")


def archive_to_huggingface(logger: PredictionLogger):
    """Push accumulated prediction logs to HuggingFace Datasets."""
    import os
    hf_token = os.environ.get("HF_TOKEN", "")
    if not hf_token:
        print("  No HF_TOKEN — skipping archival")
        return

    try:
        from huggingface_hub import HfApi
        api = HfApi(token=hf_token)

        repo_id = "DominicTanzillo/panacea-predictions"

        # Create repo if it doesn't exist
        try:
            api.create_repo(repo_id, repo_type="dataset", exist_ok=True)
        except Exception:
            pass

        # Upload all local prediction logs
        log_dir = logger.local_dir
        for f in sorted(log_dir.glob("*.jsonl")):
            api.upload_file(
                path_or_fileobj=str(f),
                path_in_repo=f"logs/{f.name}",
                repo_id=repo_id,
                repo_type="dataset",
            )
        print(f"  Archived {len(list(log_dir.glob('*.jsonl')))} log files to HuggingFace")

    except ImportError:
        print("  huggingface_hub not installed — skipping archival")
    except Exception as e:
        print(f"  HuggingFace archival error: {e}")


if __name__ == "__main__":
    main()
