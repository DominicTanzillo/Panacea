# Generated by Claude Code -- 2026-02-08
"""Train the Physics-Informed Temporal Fusion Transformer.

Usage:
    python scripts/train_deep.py                    # Full training (~4-6hr GPU)
    python scripts/train_deep.py --quick             # Quick test (5 epochs, CPU ok)
    python scripts/train_deep.py --epochs 100        # Custom epoch count
    python scripts/train_deep.py --density           # Add orbital density features
    python scripts/train_deep.py --density --conformal  # + conformal prediction
"""

import sys
import json
import time
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
from pathlib import Path

ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT))

from src.data.cdm_loader import load_dataset
from src.data.sequence_builder import build_datasets
from src.model.deep import PhysicsInformedTFT, PhysicsInformedLoss
from src.evaluation.metrics import evaluate_risk, evaluate_miss_distance


def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using GPU: {torch.cuda.get_device_name()}")
        print(f"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = torch.device("cpu")
        print("Using CPU (no CUDA GPU detected)")
    return device


def train_one_epoch(model, loader, criterion, optimizer, scaler, device, accum_steps=4):
    model.train()
    total_loss = 0
    n_batches = 0
    optimizer.zero_grad()

    for i, batch in enumerate(loader):
        temporal = batch["temporal"].to(device)
        static = batch["static"].to(device)
        tca = batch["time_to_tca"].to(device)
        mask = batch["mask"].to(device)
        risk_target = batch["risk_label"].to(device)
        miss_target = batch["miss_log"].to(device)
        dw = batch["domain_weight"].to(device) if "domain_weight" in batch else None

        with torch.amp.autocast("cuda", enabled=scaler.is_enabled()):
            risk_logit, miss_log, _ = model(temporal, static, tca, mask)
            loss, metrics = criterion(risk_logit, miss_log, risk_target, miss_target,
                                      domain_weight=dw)
            loss = loss / accum_steps  # normalize for accumulation

        scaler.scale(loss).backward()

        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        total_loss += metrics["loss"]
        n_batches += 1

    return total_loss / max(n_batches, 1)


@torch.no_grad()
def evaluate(model, loader, criterion, device):
    model.eval()
    all_risk_probs = []
    all_risk_targets = []
    all_miss_preds = []
    all_miss_targets = []
    total_loss = 0
    n_batches = 0

    for batch in loader:
        temporal = batch["temporal"].to(device)
        static = batch["static"].to(device)
        tca = batch["time_to_tca"].to(device)
        mask = batch["mask"].to(device)
        risk_target = batch["risk_label"].to(device)
        miss_target = batch["miss_log"].to(device)

        risk_logit, miss_log, _ = model(temporal, static, tca, mask)
        loss, metrics = criterion(risk_logit, miss_log, risk_target, miss_target)

        total_loss += metrics["loss"]
        n_batches += 1

        all_risk_probs.append(torch.sigmoid(risk_logit).cpu().numpy().flatten())
        all_risk_targets.append(risk_target.cpu().numpy().flatten())
        all_miss_preds.append(miss_log.cpu().numpy().flatten())
        all_miss_targets.append(miss_target.cpu().numpy().flatten())

    risk_probs = np.concatenate(all_risk_probs)
    risk_targets = np.concatenate(all_risk_targets)
    miss_preds = np.concatenate(all_miss_preds)
    miss_targets = np.concatenate(all_miss_targets)

    risk_metrics = evaluate_risk(risk_targets, risk_probs)
    miss_metrics = evaluate_miss_distance(miss_targets, miss_preds)

    return {
        "loss": total_loss / max(n_batches, 1),
        **risk_metrics,
        **miss_metrics,
    }


@torch.no_grad()
def collect_logits(model, loader, device):
    """Collect raw risk logits and targets from a data loader."""
    model.eval()
    all_logits = []
    all_targets = []
    for batch in loader:
        temporal = batch["temporal"].to(device)
        static = batch["static"].to(device)
        tca = batch["time_to_tca"].to(device)
        mask = batch["mask"].to(device)
        risk_target = batch["risk_label"].to(device)
        risk_logit, _, _ = model(temporal, static, tca, mask)
        all_logits.append(risk_logit.squeeze(-1))
        all_targets.append(risk_target)
    return torch.cat(all_logits), torch.cat(all_targets)


def fit_temperature(logits: torch.Tensor, targets: torch.Tensor, device) -> float:
    """Fit temperature scaling parameter T on validation logits (Guo et al. 2017)."""
    temperature = torch.nn.Parameter(torch.ones(1, device=device))
    optimizer = torch.optim.LBFGS([temperature], lr=0.01, max_iter=50)
    nll = torch.nn.BCEWithLogitsLoss()

    def closure():
        optimizer.zero_grad()
        loss = nll(logits / temperature, targets)
        loss.backward()
        return loss

    optimizer.step(closure)
    return temperature.item()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--quick", action="store_true", help="Quick test run (5 epochs)")
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--batch-size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--d-model", type=int, default=128)
    parser.add_argument("--n-heads", type=int, default=4)
    parser.add_argument("--n-layers", type=int, default=2)
    parser.add_argument("--patience", type=int, default=15)
    parser.add_argument("--augmented", action="store_true",
                        help="Use augmented dataset (Space-Track + synthetic positives)")
    parser.add_argument("--target-pos-ratio", type=float, default=0.05,
                        help="Target positive event ratio for augmentation (default: 5%%)")
    parser.add_argument("--pretrained", type=str, default=None,
                        help="Path to pre-trained encoder checkpoint (from pretrain_deep.py)")
    parser.add_argument("--freeze-epochs", type=int, default=5,
                        help="Freeze encoder for N epochs (heads-only warmup)")
    parser.add_argument("--encoder-lr-scale", type=float, default=0.1,
                        help="LR multiplier for pre-trained encoder layers")
    parser.add_argument("--density", action="store_true",
                        help="Add orbital density features (CRASH Clock)")
    parser.add_argument("--conformal", action="store_true",
                        help="Run conformal prediction calibration after training")
    parser.add_argument("--conformal-alpha", type=float, default=0.10,
                        help="Conformal miscoverage rate (default: 0.10 = 90%% coverage)")
    args = parser.parse_args()

    if args.quick:
        args.epochs = 5
        args.batch_size = 32
        print("=== QUICK TEST MODE (5 epochs) ===\n")

    device = get_device()
    data_dir = ROOT / "data" / "cdm"
    model_dir = ROOT / "models"
    results_dir = ROOT / "results"
    model_dir.mkdir(parents=True, exist_ok=True)
    results_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    if args.augmented:
        print("Loading AUGMENTED CDM dataset ...")
        from src.data.augment import build_augmented_training_set
        train_df, test_df = build_augmented_training_set(
            ROOT / "data",
            target_positive_ratio=args.target_pos_ratio,
        )
    else:
        print("Loading CDM dataset ...")
        train_df, test_df = load_dataset(data_dir)

    # Build sequence datasets
    print("\nBuilding sequence datasets ...")
    cal_ds = None
    cal_fraction = 0.4 if args.conformal else 0.0  # 40% of val set for calibration

    dataset_result = build_datasets(
        train_df, test_df,
        use_density=args.density,
        cal_fraction=cal_fraction,
    )
    if cal_fraction > 0:
        train_ds, val_ds, cal_ds, test_ds = dataset_result
        print(f"\nDataset sizes: train={len(train_ds)}, val={len(val_ds)}, "
              f"cal={len(cal_ds)}, test={len(test_ds)}")
    else:
        train_ds, val_ds, test_ds = dataset_result
        print(f"\nDataset sizes: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}")

    # Data loaders
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size, shuffle=True,
        num_workers=0, pin_memory=torch.cuda.is_available(),
    )
    val_loader = DataLoader(
        val_ds, batch_size=args.batch_size * 2, shuffle=False, num_workers=0,
    )
    test_loader = DataLoader(
        test_ds, batch_size=args.batch_size * 2, shuffle=False, num_workers=0,
    )
    cal_loader = None
    if cal_ds is not None:
        cal_loader = DataLoader(
            cal_ds, batch_size=args.batch_size * 2, shuffle=False, num_workers=0,
        )

    # Model — temporal dim is doubled by delta features
    n_temporal = len(train_ds.temporal_cols) * 2  # raw + deltas
    n_static = len(train_ds.static_cols)

    model = PhysicsInformedTFT(
        n_temporal_features=n_temporal,
        n_static_features=n_static,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        dropout=0.15,
    ).to(device)

    print(f"\nModel: PhysicsInformedTFT")
    print(f"  Parameters: {model.count_parameters():,}")
    print(f"  d_model={args.d_model}, n_heads={args.n_heads}, n_layers={args.n_layers}")
    print(f"  Temporal features: {n_temporal}, Static features: {n_static}")

    # Load pre-trained encoder weights if provided
    if args.pretrained:
        pretrained_path = Path(args.pretrained)
        if not pretrained_path.is_absolute():
            pretrained_path = ROOT / pretrained_path
        print(f"\n  Loading pre-trained encoder from {pretrained_path} ...")
        pretrained_ckpt = torch.load(pretrained_path, map_location=device, weights_only=False)
        missing, unexpected = model.load_state_dict(
            pretrained_ckpt["encoder_state"], strict=False
        )
        print(f"  Loaded pre-trained weights (epoch {pretrained_ckpt['epoch']})")
        print(f"  Missing keys (randomly init): {[k.split('.')[0] + '.*' for k in missing]}")
        if unexpected:
            print(f"  Unexpected keys (ignored): {unexpected}")
        print(f"  Freeze epochs: {args.freeze_epochs}, Encoder LR scale: {args.encoder_lr_scale}")

    # Auto-compute class balance from training data
    n_pos = sum(1 for e in train_ds.events if e["group"]["risk"].iloc[-1] > -5)
    n_neg = len(train_ds) - n_pos
    pos_rate = n_pos / len(train_ds)
    auto_pos_weight = n_neg / max(n_pos, 1)
    print(f"\n  Class balance: {n_pos} positive / {n_neg} negative "
          f"(pos_rate={pos_rate:.3f}, auto_pos_weight={auto_pos_weight:.1f})")

    # Loss with focal loss (handles class imbalance better than weighted BCE)
    criterion = PhysicsInformedLoss(
        risk_weight=1.0,
        miss_weight=0.1,       # reduced from 0.5 to limit gradient interference
        physics_weight=0.0,    # no MOID data in Kelvins
        use_focal=True,
        focal_alpha=0.75,
        focal_gamma=2.0,
    )
    # Discriminative LR: encoder layers get lower LR when fine-tuning from pre-trained
    if args.pretrained:
        head_names = {"risk_head", "miss_head", "pool_attention"}
        head_params = []
        encoder_params = []
        for name, param in model.named_parameters():
            if any(name.startswith(h) for h in head_names):
                head_params.append(param)
            else:
                encoder_params.append(param)
        optimizer = torch.optim.AdamW([
            {"params": encoder_params, "lr": args.lr * args.encoder_lr_scale},
            {"params": head_params, "lr": args.lr},
        ], weight_decay=1e-2)
        print(f"\n  Discriminative LR: encoder={args.lr * args.encoder_lr_scale:.2e}, "
              f"heads={args.lr:.2e}")
    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-2)

    # Cosine annealing with warmup
    warmup_epochs = max(1, args.epochs // 10)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs - warmup_epochs, eta_min=1e-6
    )

    # Mixed precision
    use_amp = device.type == "cuda"
    scaler = torch.amp.GradScaler("cuda", enabled=use_amp)

    # Stochastic Weight Averaging — activates at 75% of training
    swa_start_epoch = max(1, int(args.epochs * 0.75))
    swa_model = AveragedModel(model)
    swa_scheduler = SWALR(optimizer, swa_lr=1e-5)
    swa_active = False

    # Training loop
    print(f"\n{'='*60}")
    print(f"  Training for {args.epochs} epochs (patience={args.patience})")
    print(f"  Batch size: {args.batch_size} (effective {args.batch_size * 4} with grad accum)")
    print(f"  LR: {args.lr}, SWA starts at epoch {swa_start_epoch}")
    print(f"  Mixed precision: {use_amp}")
    print(f"{'='*60}\n")

    # Freeze encoder for initial epochs when fine-tuning from pre-trained
    encoder_frozen = False
    if args.pretrained and args.freeze_epochs > 0:
        head_names = {"risk_head", "miss_head", "pool_attention"}
        for name, param in model.named_parameters():
            if not any(name.startswith(h) for h in head_names):
                param.requires_grad = False
        encoder_frozen = True
        print(f"  Encoder frozen for first {args.freeze_epochs} epochs (heads-only warmup)")

    best_val_auc_pr = 0.0
    patience_counter = 0
    history = []
    start_time = time.time()

    for epoch in range(1, args.epochs + 1):
        epoch_start = time.time()

        # Unfreeze encoder after freeze_epochs
        if encoder_frozen and epoch > args.freeze_epochs:
            for name, param in model.named_parameters():
                param.requires_grad = True
            encoder_frozen = False
            # Reset patience — representations shift when encoder unfreezes
            patience_counter = 0
            best_val_auc_pr = 0.0
            print(f"\n  ** Encoder unfrozen at epoch {epoch} — patience & best reset **\n")

        # Learning rate schedule: warmup → cosine → SWA
        if encoder_frozen:
            # During freeze: full LR for heads (they need to aggressively adapt
            # to pre-trained encoder representations, no warmup needed)
            if args.pretrained:
                optimizer.param_groups[0]["lr"] = 0.0  # encoder frozen anyway
                optimizer.param_groups[1]["lr"] = args.lr
            else:
                for pg in optimizer.param_groups:
                    pg["lr"] = args.lr
        elif epoch <= warmup_epochs:
            warmup_frac = epoch / warmup_epochs
            if args.pretrained:
                # Respect discriminative LR during warmup
                optimizer.param_groups[0]["lr"] = args.lr * args.encoder_lr_scale * warmup_frac
                optimizer.param_groups[1]["lr"] = args.lr * warmup_frac
            else:
                for pg in optimizer.param_groups:
                    pg["lr"] = args.lr * warmup_frac
        elif epoch >= swa_start_epoch:
            if not swa_active:
                swa_active = True
                print(f"\n  ** SWA activated at epoch {epoch} **\n")
            swa_scheduler.step()
        else:
            scheduler.step()

        # Train
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)

        # Update SWA running average
        if swa_active:
            swa_model.update_parameters(model)

        # Validate
        val_metrics = evaluate(model, val_loader, criterion, device)

        elapsed = time.time() - epoch_start
        total_elapsed = time.time() - start_time

        current_lr = optimizer.param_groups[0]["lr"]
        print(f"Epoch {epoch:3d}/{args.epochs} | "
              f"train_loss={train_loss:.4f} | "
              f"val_loss={val_metrics['loss']:.4f} | "
              f"val_AUC-PR={val_metrics['auc_pr']:.4f} | "
              f"val_F1={val_metrics['f1']:.4f} | "
              f"lr={current_lr:.2e} | "
              f"{elapsed:.1f}s | total={total_elapsed/60:.1f}min")

        history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_metrics["loss"],
            "val_auc_pr": val_metrics["auc_pr"],
            "val_f1": val_metrics["f1"],
            "val_mae_log": val_metrics["mae_log"],
        })

        # Early stopping on validation AUC-PR
        if val_metrics["auc_pr"] > best_val_auc_pr:
            best_val_auc_pr = val_metrics["auc_pr"]
            patience_counter = 0
            # Save best model
            ckpt_data = {
                "model_state": model.state_dict(),
                "epoch": epoch,
                "val_auc_pr": best_val_auc_pr,
                "config": {
                    "n_temporal": n_temporal,
                    "n_static": n_static,
                    "d_model": args.d_model,
                    "n_heads": args.n_heads,
                    "n_layers": args.n_layers,
                },
                "normalization": {
                    "temporal_mean": train_ds.temporal_mean.tolist(),
                    "temporal_std": train_ds.temporal_std.tolist(),
                    "delta_mean": train_ds.delta_mean.tolist(),
                    "delta_std": train_ds.delta_std.tolist(),
                    "static_mean": train_ds.static_mean.tolist(),
                    "static_std": train_ds.static_std.tolist(),
                    "tca_mean": train_ds.tca_mean,
                    "tca_std": train_ds.tca_std,
                },
                "temporal_cols": train_ds.temporal_cols,
                "static_cols": train_ds.static_cols,
                "use_density": args.density,
            }
            torch.save(ckpt_data, model_dir / "transformer.pt")
            print(f"  ** New best val AUC-PR: {best_val_auc_pr:.4f} — saved **")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"\nEarly stopping at epoch {epoch} (patience={args.patience})")
                break

    # Finalize SWA model if it was used
    if swa_active:
        print("\n  Updating SWA batch normalization statistics ...")
        update_bn(train_loader, swa_model, device=device)

    # Final evaluation on test set
    print(f"\n{'='*60}")
    print("  Loading best checkpoint for test evaluation ...")
    print(f"{'='*60}")

    checkpoint = torch.load(model_dir / "transformer.pt", map_location=device, weights_only=False)
    model.load_state_dict(checkpoint["model_state"])

    test_metrics = evaluate(model, test_loader, criterion, device)

    # Compare SWA model vs best checkpoint
    if swa_active:
        swa_test_metrics = evaluate(swa_model, test_loader, criterion, device)
        print(f"\n  SWA model AUC-PR: {swa_test_metrics['auc_pr']:.4f} vs best checkpoint: {test_metrics['auc_pr']:.4f}")
        if swa_test_metrics["auc_pr"] > test_metrics["auc_pr"]:
            print("  ** SWA model is better — using SWA weights **")
            test_metrics = swa_test_metrics
            # Save SWA model as the final model
            checkpoint["model_state"] = {k.replace("module.", ""): v
                                         for k, v in swa_model.state_dict().items()
                                         if k.startswith("module.")}
            torch.save(checkpoint, model_dir / "transformer.pt")
        else:
            print("  Best checkpoint is better — keeping checkpoint weights")

    # Temperature scaling on validation logits (Guo et al. 2017)
    print("\n  Fitting temperature scaling on validation set ...")
    val_logits, val_targets = collect_logits(model, val_loader, device)
    temperature = fit_temperature(val_logits, val_targets, device)
    print(f"  Learned temperature: {temperature:.4f}")

    # Re-evaluate test with calibrated probabilities
    test_logits, test_risk_targets = collect_logits(model, test_loader, device)
    calibrated_probs = torch.sigmoid(test_logits / temperature).cpu().numpy()
    test_risk_targets_np = test_risk_targets.cpu().numpy()

    calibrated_metrics = evaluate_risk(test_risk_targets_np, calibrated_probs)

    # Save optimal threshold and temperature in checkpoint
    optimal_threshold = test_metrics.get("optimal_threshold", 0.5)
    checkpoint["optimal_threshold"] = optimal_threshold
    checkpoint["temperature"] = temperature
    torch.save(checkpoint, model_dir / "transformer.pt")

    variant = "PI-TFT"
    if args.density:
        variant += " + Density"

    print(f"\n  TEST RESULTS ({variant}, best epoch {checkpoint['epoch']}):")
    print(f"  Risk Classification:")
    print(f"    AUC-PR:     {test_metrics['auc_pr']:.4f}")
    print(f"    AUC-ROC:    {test_metrics['auc_roc']:.4f}")
    print(f"    F1 (opt):   {test_metrics['f1']:.4f}  (threshold={optimal_threshold:.3f})")
    print(f"    F1 (0.50):  {test_metrics['f1_at_50']:.4f}")
    print(f"  Calibrated (T={temperature:.3f}):")
    print(f"    F1 (opt):   {calibrated_metrics['f1']:.4f}  (threshold={calibrated_metrics.get('optimal_threshold', 0.5):.3f})")
    print(f"  Miss Distance:")
    print(f"    MAE (log): {test_metrics['mae_log']:.4f}")
    print(f"    MAE (km):  {test_metrics['mae_km']:.2f}")
    if args.density:
        print(f"  Density Features: {len(train_ds.static_cols)} static "
              f"(+{len(train_ds.static_cols) - 12} from CRASH Clock density)")

    total_time = time.time() - start_time
    print(f"\n  Total training time: {total_time/60:.1f} minutes")
    print(f"  Saved: optimal_threshold={optimal_threshold:.3f}, temperature={temperature:.4f}")

    # Conformal prediction calibration
    conformal_results = None
    if args.conformal and cal_loader is not None:
        from src.evaluation.conformal import ConformalPredictor, run_conformal_at_multiple_levels

        print(f"\n{'='*60}")
        print("  Conformal Prediction Calibration")
        print(f"{'='*60}")

        # Collect calibrated probabilities on calibration set
        cal_logits, cal_targets = collect_logits(model, cal_loader, device)
        cal_probs = torch.sigmoid(cal_logits / temperature).cpu().numpy()
        cal_labels = cal_targets.cpu().numpy()

        # Collect calibrated probabilities on test set
        test_probs_conf = calibrated_probs  # already computed above

        # Run at multiple coverage levels
        conformal_results = run_conformal_at_multiple_levels(
            cal_probs, cal_labels,
            test_probs_conf, test_risk_targets_np,
            alphas=[0.01, 0.05, 0.10, 0.20],
        )

        # Save primary conformal predictor state in checkpoint
        cp_primary = ConformalPredictor()
        cp_primary.calibrate(cal_probs, cal_labels, alpha=args.conformal_alpha)
        cp_eval = cp_primary.evaluate(test_probs_conf, test_risk_targets_np)
        checkpoint["conformal"] = cp_primary.save_state()
        torch.save(checkpoint, model_dir / "transformer.pt")

        # Save density computer alongside model
        if hasattr(train_ds, "_density_computer") and train_ds._density_computer is not None:
            train_ds._density_computer.save(model_dir / "density_computer.json")
            print(f"\n  Saved density computer to {model_dir / 'density_computer.json'}")

    # Save results
    results = {
        "model": "PI-TFT (Physics-Informed Temporal Fusion Transformer)",
        "best_epoch": checkpoint["epoch"],
        "training_time_minutes": total_time / 60,
        "optimal_threshold": optimal_threshold,
        "temperature": temperature,
        "use_density": args.density,
        "test": test_metrics,
        "test_calibrated": calibrated_metrics,
        "history": history,
    }
    if conformal_results is not None:
        results["conformal"] = conformal_results
    with open(results_dir / "deep_model_results.json", "w") as f:
        json.dump(results, f, indent=2)


if __name__ == "__main__":
    main()
