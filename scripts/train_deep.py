# Generated by Claude Code -- 2026-02-08
"""Train the Physics-Informed Temporal Fusion Transformer.

Usage:
    python scripts/train_deep.py                    # Full training (~4-6hr GPU)
    python scripts/train_deep.py --quick             # Quick test (5 epochs, CPU ok)
    python scripts/train_deep.py --epochs 100        # Custom epoch count
"""

import sys
import json
import time
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from pathlib import Path

ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT))

from src.data.cdm_loader import load_dataset
from src.data.sequence_builder import build_datasets
from src.model.deep import PhysicsInformedTFT, PhysicsInformedLoss
from src.evaluation.metrics import evaluate_risk, evaluate_miss_distance


def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using GPU: {torch.cuda.get_device_name()}")
        print(f"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = torch.device("cpu")
        print("Using CPU (no CUDA GPU detected)")
    return device


def train_one_epoch(model, loader, criterion, optimizer, scaler, device):
    model.train()
    total_loss = 0
    n_batches = 0

    for batch in loader:
        temporal = batch["temporal"].to(device)
        static = batch["static"].to(device)
        tca = batch["time_to_tca"].to(device)
        mask = batch["mask"].to(device)
        risk_target = batch["risk_label"].to(device)
        miss_target = batch["miss_log"].to(device)

        optimizer.zero_grad()

        with torch.amp.autocast("cuda", enabled=scaler.is_enabled()):
            risk_logit, miss_log, _ = model(temporal, static, tca, mask)
            loss, metrics = criterion(risk_logit, miss_log, risk_target, miss_target)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        total_loss += metrics["loss"]
        n_batches += 1

    return total_loss / max(n_batches, 1)


@torch.no_grad()
def evaluate(model, loader, criterion, device):
    model.eval()
    all_risk_probs = []
    all_risk_targets = []
    all_miss_preds = []
    all_miss_targets = []
    total_loss = 0
    n_batches = 0

    for batch in loader:
        temporal = batch["temporal"].to(device)
        static = batch["static"].to(device)
        tca = batch["time_to_tca"].to(device)
        mask = batch["mask"].to(device)
        risk_target = batch["risk_label"].to(device)
        miss_target = batch["miss_log"].to(device)

        risk_logit, miss_log, _ = model(temporal, static, tca, mask)
        loss, metrics = criterion(risk_logit, miss_log, risk_target, miss_target)

        total_loss += metrics["loss"]
        n_batches += 1

        all_risk_probs.append(torch.sigmoid(risk_logit).cpu().numpy().flatten())
        all_risk_targets.append(risk_target.cpu().numpy().flatten())
        all_miss_preds.append(miss_log.cpu().numpy().flatten())
        all_miss_targets.append(miss_target.cpu().numpy().flatten())

    risk_probs = np.concatenate(all_risk_probs)
    risk_targets = np.concatenate(all_risk_targets)
    miss_preds = np.concatenate(all_miss_preds)
    miss_targets = np.concatenate(all_miss_targets)

    risk_metrics = evaluate_risk(risk_targets, risk_probs)
    miss_metrics = evaluate_miss_distance(miss_targets, miss_preds)

    return {
        "loss": total_loss / max(n_batches, 1),
        **risk_metrics,
        **miss_metrics,
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--quick", action="store_true", help="Quick test run (5 epochs)")
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--batch-size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--d-model", type=int, default=128)
    parser.add_argument("--n-heads", type=int, default=4)
    parser.add_argument("--n-layers", type=int, default=2)
    parser.add_argument("--patience", type=int, default=15)
    args = parser.parse_args()

    if args.quick:
        args.epochs = 5
        args.batch_size = 32
        print("=== QUICK TEST MODE (5 epochs) ===\n")

    device = get_device()
    data_dir = ROOT / "data" / "cdm"
    model_dir = ROOT / "models"
    results_dir = ROOT / "results"
    model_dir.mkdir(parents=True, exist_ok=True)
    results_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    print("Loading CDM dataset ...")
    train_df, test_df = load_dataset(data_dir)

    # Build sequence datasets
    print("\nBuilding sequence datasets ...")
    train_ds, val_ds, test_ds = build_datasets(train_df, test_df)

    print(f"\nDataset sizes: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}")

    # Data loaders
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size, shuffle=True,
        num_workers=0, pin_memory=torch.cuda.is_available(),
    )
    val_loader = DataLoader(
        val_ds, batch_size=args.batch_size * 2, shuffle=False, num_workers=0,
    )
    test_loader = DataLoader(
        test_ds, batch_size=args.batch_size * 2, shuffle=False, num_workers=0,
    )

    # Model
    n_temporal = len(train_ds.temporal_cols)
    n_static = len(train_ds.static_cols)

    model = PhysicsInformedTFT(
        n_temporal_features=n_temporal,
        n_static_features=n_static,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        dropout=0.15,
    ).to(device)

    print(f"\nModel: PhysicsInformedTFT")
    print(f"  Parameters: {model.count_parameters():,}")
    print(f"  d_model={args.d_model}, n_heads={args.n_heads}, n_layers={args.n_layers}")
    print(f"  Temporal features: {n_temporal}, Static features: {n_static}")

    # Loss, optimizer, scheduler
    criterion = PhysicsInformedLoss(
        risk_weight=1.0, miss_weight=0.5, physics_weight=0.0,  # no MOID data in Kelvins
        pos_weight=50.0,
    )
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-2)

    # Cosine annealing with warmup
    warmup_epochs = max(1, args.epochs // 10)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs - warmup_epochs, eta_min=1e-6
    )

    # Mixed precision
    use_amp = device.type == "cuda"
    scaler = torch.amp.GradScaler("cuda", enabled=use_amp)

    # Training loop
    print(f"\n{'='*60}")
    print(f"  Training for {args.epochs} epochs (patience={args.patience})")
    print(f"  Batch size: {args.batch_size}, LR: {args.lr}")
    print(f"  Mixed precision: {use_amp}")
    print(f"{'='*60}\n")

    best_val_auc_pr = 0.0
    patience_counter = 0
    history = []
    start_time = time.time()

    for epoch in range(1, args.epochs + 1):
        epoch_start = time.time()

        # Warmup: linearly increase LR for first N epochs
        if epoch <= warmup_epochs:
            warmup_lr = args.lr * (epoch / warmup_epochs)
            for pg in optimizer.param_groups:
                pg["lr"] = warmup_lr
        else:
            scheduler.step()

        # Train
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)

        # Validate
        val_metrics = evaluate(model, val_loader, criterion, device)

        elapsed = time.time() - epoch_start
        total_elapsed = time.time() - start_time

        current_lr = optimizer.param_groups[0]["lr"]
        print(f"Epoch {epoch:3d}/{args.epochs} | "
              f"train_loss={train_loss:.4f} | "
              f"val_loss={val_metrics['loss']:.4f} | "
              f"val_AUC-PR={val_metrics['auc_pr']:.4f} | "
              f"val_F1={val_metrics['f1']:.4f} | "
              f"lr={current_lr:.2e} | "
              f"{elapsed:.1f}s | total={total_elapsed/60:.1f}min")

        history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_metrics["loss"],
            "val_auc_pr": val_metrics["auc_pr"],
            "val_f1": val_metrics["f1"],
            "val_mae_log": val_metrics["mae_log"],
        })

        # Early stopping on validation AUC-PR
        if val_metrics["auc_pr"] > best_val_auc_pr:
            best_val_auc_pr = val_metrics["auc_pr"]
            patience_counter = 0
            # Save best model
            torch.save({
                "model_state": model.state_dict(),
                "epoch": epoch,
                "val_auc_pr": best_val_auc_pr,
                "config": {
                    "n_temporal": n_temporal,
                    "n_static": n_static,
                    "d_model": args.d_model,
                    "n_heads": args.n_heads,
                    "n_layers": args.n_layers,
                },
                "normalization": {
                    "temporal_mean": train_ds.temporal_mean.tolist(),
                    "temporal_std": train_ds.temporal_std.tolist(),
                    "static_mean": train_ds.static_mean.tolist(),
                    "static_std": train_ds.static_std.tolist(),
                    "tca_mean": train_ds.tca_mean,
                    "tca_std": train_ds.tca_std,
                },
                "temporal_cols": train_ds.temporal_cols,
                "static_cols": train_ds.static_cols,
            }, model_dir / "transformer.pt")
            print(f"  ** New best val AUC-PR: {best_val_auc_pr:.4f} â€” saved **")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"\nEarly stopping at epoch {epoch} (patience={args.patience})")
                break

    # Final evaluation on test set
    print(f"\n{'='*60}")
    print("  Loading best model for test evaluation ...")
    print(f"{'='*60}")

    checkpoint = torch.load(model_dir / "transformer.pt", map_location=device, weights_only=False)
    model.load_state_dict(checkpoint["model_state"])

    test_metrics = evaluate(model, test_loader, criterion, device)

    print(f"\n  TEST RESULTS (PI-TFT, best epoch {checkpoint['epoch']}):")
    print(f"  Risk Classification:")
    print(f"    AUC-PR:  {test_metrics['auc_pr']:.4f}")
    print(f"    AUC-ROC: {test_metrics['auc_roc']:.4f}")
    print(f"    F1:      {test_metrics['f1']:.4f}")
    print(f"  Miss Distance:")
    print(f"    MAE (log): {test_metrics['mae_log']:.4f}")
    print(f"    MAE (km):  {test_metrics['mae_km']:.2f}")

    total_time = time.time() - start_time
    print(f"\n  Total training time: {total_time/60:.1f} minutes")

    # Save results
    results = {
        "model": "PI-TFT (Physics-Informed Temporal Fusion Transformer)",
        "best_epoch": checkpoint["epoch"],
        "training_time_minutes": total_time / 60,
        "test": test_metrics,
        "history": history,
    }
    with open(results_dir / "deep_model_results.json", "w") as f:
        json.dump(results, f, indent=2)


if __name__ == "__main__":
    main()
