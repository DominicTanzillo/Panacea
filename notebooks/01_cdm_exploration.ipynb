{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the ESA Conjunction Data Messages (CDM) Dataset\n",
    "\n",
    "So I found this really cool dataset from the European Space Agency. They released real conjunction data messages from their space debris monitoring operations as part of a collision avoidance challenge. This is actual operational data from 2015-2019 that was used to make real decisions about whether satellites needed to dodge debris.\n",
    "\n",
    "The idea is that when two objects in orbit are predicted to pass close to each other, the tracking system generates a series of CDMs over the days leading up to the closest approach. Each CDM refines the prediction as more tracking data comes in. The question is -- can we predict whether a conjunction will be dangerous enough to require action?\n",
    "\n",
    "Let's see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='deep')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "DATA_DIR = Path('../data/cdm')\n",
    "print('Files available:', list(DATA_DIR.glob('*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(DATA_DIR / 'train_data.csv')\n",
    "print(f'Training set: {train.shape[0]:,} rows, {train.shape[1]} columns')\n",
    "print(f'\\nColumn names ({len(train.columns)}):')\n",
    "for i, col in enumerate(train.columns):\n",
    "    print(f'  {i:3d}. {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at the data structure\n",
    "\n",
    "103 columns is a lot. Let me get a sense of what types of features we have before diving into individual ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of data types and missing values\n",
    "print('Data types:')\n",
    "print(train.dtypes.value_counts())\n",
    "print(f'\\nTotal missing values: {train.isnull().sum().sum():,}')\n",
    "print(f'Columns with any missing: {(train.isnull().sum() > 0).sum()}')\n",
    "\n",
    "# Which columns have missing data?\n",
    "missing = train.isnull().sum()\n",
    "missing_cols = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing_cols) > 0:\n",
    "    print(f'\\nMissing value breakdown:')\n",
    "    for col, count in missing_cols.items():\n",
    "        print(f'  {col}: {count:,} ({100*count/len(train):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the event structure\n",
    "\n",
    "Each row is a single CDM (conjunction data message), but multiple CDMs belong to the same \"event\" -- meaning the same pair of objects approaching each other. The CDMs within an event form a time series as predictions get refined closer to the time of closest approach (TCA).\n",
    "\n",
    "Let me figure out how events are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's an event_id column or similar\n",
    "event_cols = [c for c in train.columns if 'event' in c.lower() or 'id' in c.lower()]\n",
    "print('Columns that might identify events:', event_cols)\n",
    "\n",
    "# Also look for time-related columns\n",
    "time_cols = [c for c in train.columns if 'time' in c.lower() or 'tca' in c.lower() or 'date' in c.lower()]\n",
    "print('Time-related columns:', time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's figure out how many unique events there are\n",
    "# and how many CDMs per event\n",
    "event_col = None\n",
    "for candidate in ['event_id', 'event', 'conjunction_id']:\n",
    "    if candidate in train.columns:\n",
    "        event_col = candidate\n",
    "        break\n",
    "\n",
    "if event_col is None:\n",
    "    # Try to find it by looking for columns with the right cardinality\n",
    "    # We expect ~13k unique events in 162k rows\n",
    "    for col in train.columns:\n",
    "        nunique = train[col].nunique()\n",
    "        if 10000 < nunique < 20000:\n",
    "            print(f'Potential event column: {col} ({nunique:,} unique values)')\n",
    "\n",
    "print(f'\\nUsing event column: {event_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of CDMs per event\n",
    "# Need to identify the event grouping column first\n",
    "# Let's look at all columns and their unique counts to find it\n",
    "print('Unique value counts for each column:')\n",
    "for col in train.columns:\n",
    "    n = train[col].nunique()\n",
    "    print(f'  {col}: {n:,} unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the target variable\n",
    "\n",
    "The Kelvins challenge was about predicting collision risk. Let me find the target column and understand the class distribution. Since this is a safety-critical problem, I expect the classes to be heavily imbalanced -- most conjunctions are safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for target/risk/label columns\n",
    "target_candidates = [c for c in train.columns if any(\n",
    "    kw in c.lower() for kw in ['risk', 'label', 'target', 'class', 'danger', 'collision']\n",
    ")]\n",
    "print('Potential target columns:', target_candidates)\n",
    "\n",
    "# For each candidate, show value distribution\n",
    "for col in target_candidates:\n",
    "    print(f'\\n{col}:')\n",
    "    print(train[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at miss_distance -- this is the key physical quantity\n",
    "miss_cols = [c for c in train.columns if 'miss' in c.lower() or 'distance' in c.lower()]\n",
    "print('Miss distance related columns:', miss_cols)\n",
    "\n",
    "for col in miss_cols:\n",
    "    if train[col].dtype in ['float64', 'float32', 'int64']:\n",
    "        print(f'\\n{col}:')\n",
    "        print(f'  min:    {train[col].min():.6f}')\n",
    "        print(f'  median: {train[col].median():.6f}')\n",
    "        print(f'  mean:   {train[col].mean():.6f}')\n",
    "        print(f'  max:    {train[col].max():.6f}')\n",
    "        print(f'  std:    {train[col].std():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miss distance distribution\n",
    "\n",
    "Miss distance is super important -- it's the predicted closest approach between two objects. Small miss distance = scary. Let me visualize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the main miss distance column\n",
    "miss_col = None\n",
    "for candidate in ['miss_distance', 'MISS_DISTANCE', 'miss_dist']:\n",
    "    if candidate in train.columns:\n",
    "        miss_col = candidate\n",
    "        break\n",
    "\n",
    "# If we can't find an exact match, look for the most likely one\n",
    "if miss_col is None:\n",
    "    for col in miss_cols:\n",
    "        if train[col].dtype in ['float64', 'float32']:\n",
    "            miss_col = col\n",
    "            break\n",
    "\n",
    "if miss_col:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Raw distribution\n",
    "    axes[0].hist(train[miss_col].dropna(), bins=100, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Miss Distance')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Miss Distance Distribution (raw)')\n",
    "    \n",
    "    # Log-scale -- miss distances often span orders of magnitude\n",
    "    log_miss = np.log10(train[miss_col].dropna().clip(lower=1e-10))\n",
    "    axes[1].hist(log_miss, bins=100, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].set_xlabel('log10(Miss Distance)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Miss Distance Distribution (log scale)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Could not identify miss distance column. Columns available:')\n",
    "    print([c for c in train.columns if train[c].dtype in ['float64', 'float32']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-to-TCA analysis\n",
    "\n",
    "One of the most interesting aspects of this dataset is the temporal structure. CDMs arrive at different times before the predicted closest approach. Earlier CDMs have more uncertainty, later ones are more precise. How does the prediction quality evolve over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the time_to_tca column\n",
    "tca_col = None\n",
    "for candidate in ['time_to_tca', 'TIME_TO_TCA', 'days_to_tca', 't_to_tca']:\n",
    "    if candidate in train.columns:\n",
    "        tca_col = candidate\n",
    "        break\n",
    "\n",
    "# If not found, search more broadly\n",
    "if tca_col is None:\n",
    "    for col in time_cols:\n",
    "        if train[col].dtype in ['float64', 'float32'] and train[col].min() >= 0:\n",
    "            print(f'Candidate time column: {col}')\n",
    "            print(f'  Range: [{train[col].min():.4f}, {train[col].max():.4f}]')\n",
    "            print(f'  Mean: {train[col].mean():.4f}')\n",
    "            tca_col = col  # take the first reasonable one\n",
    "\n",
    "if tca_col:\n",
    "    print(f'Using time-to-TCA column: {tca_col}')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.hist(train[tca_col].dropna(), bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax.set_xlabel('Time to TCA (days)')\n",
    "    ax.set_ylabel('Number of CDMs')\n",
    "    ax.set_title('When do CDMs arrive relative to closest approach?')\n",
    "    ax.axvline(x=1.0, color='red', linestyle='--', alpha=0.7, label='1 day before TCA')\n",
    "    ax.axvline(x=3.0, color='orange', linestyle='--', alpha=0.7, label='3 days before TCA')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object types\n",
    "\n",
    "Not all space objects are the same. There are active satellites (payloads), rocket bodies left over from launches, and debris fragments from collisions or explosions. The type of objects involved in a conjunction matters a lot -- two active satellites can both maneuver, but debris can't dodge.\n",
    "\n",
    "Let me see what object types are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find object type columns\n",
    "type_cols = [c for c in train.columns if 'type' in c.lower() or 'object' in c.lower()]\n",
    "print('Object-related columns:', type_cols)\n",
    "\n",
    "for col in type_cols:\n",
    "    if train[col].nunique() < 20:  # likely categorical\n",
    "        print(f'\\n{col} value counts:')\n",
    "        print(train[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we found object types, let's see how risk varies by type pair\n",
    "obj_type_cols = [c for c in type_cols if train[c].nunique() < 10]\n",
    "\n",
    "if len(obj_type_cols) >= 1 and len(target_candidates) >= 1:\n",
    "    obj_col = obj_type_cols[0]\n",
    "    tgt_col = target_candidates[0]\n",
    "    \n",
    "    # Cross-tabulate object type vs risk\n",
    "    ct = pd.crosstab(train[obj_col], train[tgt_col], normalize='index')\n",
    "    print(f'Risk rate by {obj_col}:')\n",
    "    print(ct)\n",
    "    \n",
    "    ct.plot(kind='bar', stacked=True, figsize=(10, 5))\n",
    "    plt.title(f'Risk Distribution by {obj_col}')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis\n",
    "\n",
    "With 103 features, there's probably a lot of redundancy. Let me look at correlations between the numeric features to understand the structure better and identify which features might be most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns only\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Number of numeric features: {len(numeric_cols)}')\n",
    "\n",
    "# Correlation matrix -- might be big so let's just look at the top correlations\n",
    "corr = train[numeric_cols].corr()\n",
    "\n",
    "# Find the most correlated feature pairs (excluding self-correlation)\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr = []\n",
    "for col in upper.columns:\n",
    "    for idx in upper.index:\n",
    "        val = upper.loc[idx, col]\n",
    "        if abs(val) > 0.9:\n",
    "            high_corr.append((idx, col, val))\n",
    "\n",
    "high_corr.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "print(f'\\nHighly correlated pairs (|r| > 0.9): {len(high_corr)}')\n",
    "for a, b, r in high_corr[:20]:\n",
    "    print(f'  {a} <-> {b}: {r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of a subset of interesting features\n",
    "# Pick features that seem most relevant to conjunction risk\n",
    "interesting_keywords = ['miss', 'mahalanobis', 'relative', 'collision', 'risk',\n",
    "                         'time', 'tca', 'cov', 'semi', 'ecc', 'inc']\n",
    "\n",
    "interesting_cols = [c for c in numeric_cols if any(\n",
    "    kw in c.lower() for kw in interesting_keywords\n",
    ")]\n",
    "\n",
    "if len(interesting_cols) > 3:\n",
    "    # Limit to ~20 for readability\n",
    "    interesting_cols = interesting_cols[:20]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    sub_corr = train[interesting_cols].corr()\n",
    "    sns.heatmap(sub_corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "                square=True, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    ax.set_title('Correlation Matrix - Key Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at individual event trajectories\n",
    "\n",
    "This is the part I'm most curious about. Each event is a time series of CDM updates. As we get closer to the time of closest approach, the miss distance estimate should converge. But does it converge smoothly, or does it jump around?\n",
    "\n",
    "Let me pick a few events and plot their CDM sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to identify the event grouping and then plot trajectories\n",
    "# Let's check what column groups CDMs into events\n",
    "print('Columns and sample values from first 5 rows:')\n",
    "print(train.iloc[:5, :10].to_string())\n",
    "print('...')\n",
    "print(train.iloc[:5, -10:].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we identify the event column, plot some example trajectories\n",
    "# This is a placeholder that adapts to whatever the event column turns out to be\n",
    "\n",
    "# Try to find grouping -- look for columns where consecutive rows share values\n",
    "# (CDMs from same event should be grouped together in the CSV)\n",
    "potential_group_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object' or train[col].nunique() < 20000:\n",
    "        # Check if consecutive rows often share values\n",
    "        same_as_next = (train[col] == train[col].shift(1)).mean()\n",
    "        if 0.5 < same_as_next < 0.99:\n",
    "            potential_group_cols.append((col, same_as_next, train[col].nunique()))\n",
    "\n",
    "potential_group_cols.sort(key=lambda x: x[2])\n",
    "print('Potential grouping columns (sorted by unique count):')\n",
    "for col, pct, nunique in potential_group_cols:\n",
    "    print(f'  {col}: {nunique:,} unique, {pct:.1%} consecutive matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CDM trajectories for a sample of events\n",
    "# This will adapt once we know the exact column names\n",
    "\n",
    "def plot_event_trajectories(df, event_col, time_col, value_col, n_events=6):\n",
    "    \"\"\"Plot how a value evolves across CDM updates for sample events.\"\"\"\n",
    "    events = df[event_col].unique()\n",
    "    # Pick events with at least 5 CDMs for interesting trajectories\n",
    "    event_sizes = df.groupby(event_col).size()\n",
    "    good_events = event_sizes[event_sizes >= 5].index\n",
    "    sample = np.random.choice(good_events, size=min(n_events, len(good_events)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, eid in enumerate(sample):\n",
    "        event_data = df[df[event_col] == eid].sort_values(time_col, ascending=False)\n",
    "        axes[i].plot(event_data[time_col], event_data[value_col], 'o-', markersize=4)\n",
    "        axes[i].set_xlabel('Time to TCA (days)')\n",
    "        axes[i].set_ylabel(value_col)\n",
    "        axes[i].set_title(f'Event {eid} ({len(event_data)} CDMs)')\n",
    "        axes[i].invert_xaxis()  # Time counts down to TCA\n",
    "    \n",
    "    plt.suptitle(f'CDM Trajectories: How {value_col} evolves before closest approach', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# We'll call this once we confirm the column names\n",
    "print('Ready to plot -- just need to confirm event/time/value column names')\n",
    "print(f'Candidate event cols: {[x[0] for x in potential_group_cols[:3]]}')\n",
    "print(f'Candidate time col: {tca_col}')\n",
    "print(f'Candidate value col: {miss_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just try plotting with our best guesses\n",
    "if potential_group_cols and tca_col and miss_col:\n",
    "    event_id_col = potential_group_cols[0][0]\n",
    "    print(f'Plotting with: event={event_id_col}, time={tca_col}, value={miss_col}')\n",
    "    np.random.seed(42)\n",
    "    plot_event_trajectories(train, event_id_col, tca_col, miss_col)\n",
    "else:\n",
    "    print('Need to manually identify column names first.')\n",
    "    print('Available columns:', list(train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of CDM dataset\n",
    "\n",
    "Initial takeaways from exploring this data:\n",
    "\n",
    "1. **Scale**: ~162k CDM records across ~13k unique conjunction events\n",
    "2. **Features**: 103 columns covering orbital elements, miss distance components, covariance matrices, relative velocities, and metadata\n",
    "3. **Temporal structure**: Each event is a sequence of CDMs with decreasing time-to-TCA. Average ~12 CDMs per event.\n",
    "4. **Class imbalance**: Expected to be severe -- most conjunctions are safe. This is realistic and important to handle correctly.\n",
    "5. **Miss distances span orders of magnitude**: Will need log-scale treatment for regression.\n",
    "\n",
    "Next steps:\n",
    "- Look at the CelesTrak TLE data for the visualization side\n",
    "- Understand the covariance matrix features (these encode prediction uncertainty)\n",
    "- Start thinking about feature engineering for the XGBoost model\n",
    "- Figure out the sequence padding/truncation strategy for the Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}