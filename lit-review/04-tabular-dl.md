<!-- Generated by Claude Code — 2026-02-09 -->

# Trees vs. Deep Learning on Tabular Data

## Overview

The question of whether deep learning can beat gradient boosted trees on tabular data is one of the most actively studied questions in applied machine learning. The consensus from multiple large-scale benchmarks is clear: **tree-based models remain the default best choice for typical tabular datasets**, especially when the dataset is small-to-medium sized. This directly explains Panacea's results.

---

## Key Papers

### Grinsztajn et al. (2022) — "Why do tree-based models still outperform deep learning on typical tabular data?"

**Venue**: NeurIPS 2022
**Methodology**: Benchmarked tree-based models (XGBoost, Random Forest, GBM) against deep learning models (MLP, ResNet, FT-Transformer, SAINT) across 45 datasets with a rigorous evaluation protocol including hyperparameter tuning for all methods.

**Key findings**:

1. **Trees are better at handling irregular and uninformative features.** Decision trees naturally ignore irrelevant features by not splitting on them. Neural networks must learn to ignore them through training, which requires more data and regularization.

2. **Trees are robust to feature scale.** XGBoost is invariant to monotonic transformations of features (only split orderings matter). Neural networks are sensitive to feature scaling — poor normalization directly degrades performance.

3. **Trees handle heterogeneous features naturally.** Tabular data often mixes very different feature types (counts, ratios, categorical, continuous). Trees handle this natively. Neural networks need careful preprocessing and embedding strategies.

4. **Trees learn sharp, axis-aligned decision boundaries efficiently.** Many tabular prediction tasks have decision boundaries that align with feature axes (e.g., "if miss_distance < X AND covariance_det > Y, then high risk"). Trees represent this directly; neural networks need many parameters to approximate axis-aligned splits.

5. **Neural networks need data to learn representations.** The core advantage of deep learning (learned representations) requires sufficient data to discover useful representations. On small datasets, hand-crafted features + trees outperform end-to-end learning.

**Relevance to Panacea**: The CDM dataset exemplifies nearly every characteristic that favors trees: heterogeneous features (mix of orbital mechanics, covariance statistics, space weather indices), uninformative features (many of the 103 columns have low predictive value), and small dataset size (13K events).

---

### McElfresh et al. (2023) — "When Do Neural Nets Outperform Boosted Trees on Tabular Data?"

**Venue**: NeurIPS 2023
**Methodology**: Large-scale meta-analysis across 176 datasets to identify *when* neural networks actually win.

**Key findings — Neural nets win when**:

1. **Dataset size is large (>50K samples).** Below this threshold, trees almost always win. Above 50K, neural networks become increasingly competitive. Above 500K, neural networks often dominate.

2. **Features are highly correlated.** Trees split on one feature at a time and struggle with complex feature interactions that span many correlated dimensions. Neural networks can learn linear combinations that capture these patterns.

3. **Natural representations exist.** If the tabular data contains embedded images, text, or time series, neural networks can learn representations end-to-end. Trees require these to be manually featurized first.

4. **The prediction function is smooth.** When the true decision boundary is a smooth nonlinear surface (rather than axis-aligned steps), neural networks can represent it more efficiently.

**Key findings — Trees win when**:

1. **Dataset size is small to medium (<50K).** This is the dominant factor.
2. **Features are heterogeneous and independently informative.**
3. **Noise levels are moderate to high.** Trees are more robust to label noise.
4. **Feature engineering is possible.** When domain experts can craft meaningful features, trees exploit them more effectively.

**Relevance to Panacea**: The Kelvins dataset has **~13K events** — firmly below the 50K threshold where neural networks become competitive. Combined with heterogeneous features and the availability of domain-informed feature engineering, this places CDM data squarely in tree-favored territory.

---

### Gorishniy et al. (2021) — "Revisiting Deep Learning Models for Tabular Data"

**Venue**: NeurIPS 2021
**Contribution**: Introduced the **FT-Transformer** (Feature Tokenizer + Transformer) — a Transformer architecture specifically designed for tabular data.

**Key findings**:

- FT-Transformer is **competitive** with XGBoost on many datasets but **not consistently dominant**.
- Performance depends heavily on dataset characteristics. On larger datasets with correlated features, FT-Transformer can match or beat XGBoost. On smaller, noisier datasets, XGBoost retains the advantage.
- The Feature Tokenizer (which embeds each feature into a learned token) is a key innovation, but it adds parameters that need data to train.
- **Hyperparameter sensitivity**: FT-Transformer performance varies more with hyperparameter choices than XGBoost, which is relatively robust to its settings.

**Relevance to Panacea**: Our PI-TFT architecture draws on similar ideas (attention over feature representations + temporal tokens). The finding that FT-Transformer is competitive but not dominant — and particularly that it struggles on smaller datasets — directly predicts our results.

---

### Kadra et al. (2021) — "Well-tuned Simple Nets Can Be Competitive"

**Venue**: AutoML Conference
**Contribution**: Showed that a simple MLP with a carefully chosen "regularization cocktail" (dropout + weight decay + batch normalization + learning rate scheduling + mixup + cutmix for tabular data) can be competitive with XGBoost.

**Key findings**:

- The gap between neural networks and trees is partly a **tuning gap**, not a fundamental capability gap. Default neural network training setups are poor for tabular data.
- With exhaustive regularization tuning, simple MLPs close much of the gap with XGBoost.
- However, even with optimal regularization, trees maintain an edge on small datasets and datasets with many uninformative features.
- The practical implication: neural networks *can* be made competitive, but the effort required is substantially higher than "install XGBoost and run."

**Relevance to Panacea**: Suggests that PI-TFT performance could be improved with better regularization (dropout tuning, weight decay, SWA, etc.), but likely not enough to match XGBoost on 13K events. The training techniques review (05-training-techniques.md) covers specific approaches.

---

## The Dataset Size Effect

Synthesizing across all four papers, the relationship between dataset size and optimal model choice is remarkably consistent:

| Dataset Size | Recommended Approach | Confidence |
|-------------|---------------------|------------|
| <5K samples | Trees (XGBoost/LightGBM) | Very high |
| 5K-50K samples | Trees, unless features are highly correlated | High |
| 50K-500K samples | Either; depends on feature structure | Medium |
| >500K samples | Neural networks increasingly favored | Medium-High |
| >1M samples | Neural networks likely win | High |

**Kelvins CDM dataset: ~13K events. This is firmly in the "trees win" zone.**

---

## Why This Matters for Panacea

The tabular data literature provides the theoretical grounding for what we observe empirically:

1. **XGBoost's AUC-PR of 0.978 is expected.** Trees excel on exactly this type of data.
2. **PI-TFT's AUC-PR of 0.50 is not an implementation failure.** It reflects the well-documented difficulty of training attention-based models on small tabular datasets.
3. **The three-model comparison is pedagogically valuable.** Panacea demonstrates a key lesson from the tabular data literature: model sophistication does not guarantee better performance. The best model depends on the data, not the number of parameters.
4. **There is room for PI-TFT improvement.** The literature suggests that better regularization, training techniques, and architectural choices can narrow the gap (see 05-training-techniques.md). A realistic target is AUC-PR 0.70-0.80 — respectable for a deep learning model on this data, but not competitive with XGBoost.
