<!-- Generated by Claude Code — 2026-02-09 -->

# ESA Kelvins Collision Avoidance Challenge

## Overview

The ESA Kelvins Spacecraft Collision Avoidance Challenge was a machine learning competition hosted on the Kelvins platform to advance automated conjunction assessment. The challenge attracted **73 teams** and tasked participants with predicting the collision risk (log10 collision probability) from sequences of Conjunction Data Messages (CDMs).

**Dataset**: 162,634 training CDMs grouped into 13,154 events; 24,484 test CDMs across 2,167 events.
**Metric**: RMSE of predicted log collision probability.
**Target column**: `risk` (log10 collision probability).

---

## Key Papers

### Uriot et al. (2022) — "Spacecraft Collision Avoidance Challenge: Design and results of a machine learning competition"

This is the official competition summary paper, authored by the ESA organizers (Thomas Uriot, Dario Izzo, and colleagues).

**Key findings**:
- **Gradient boosting dominated the competition.** The top solutions overwhelmingly relied on XGBoost and LightGBM, not neural networks or sequence models.
- The **winning solution by team "Predictive Layer"** used an ensemble of gradient boosted trees with extensive feature engineering.
- Top teams invested heavily in **temporal feature engineering** — computing deltas (differences between consecutive CDMs), moving averages, rates of change in miss distance, and trends in covariance matrix evolution.
- Feature engineering on the raw 103 columns proved far more valuable than architectural novelty. Teams that engineered 200-500+ features from the raw data consistently outperformed those that used the columns as-is.
- The competition highlighted that **domain-informed feature construction** (e.g., covariance ratios, relative velocity decomposition trends) was the primary differentiator between top and middle-tier solutions.

**Relevance to Panacea**: Validates XGBoost as the correct choice for the classical ML model. Our temporal feature engineering (deltas, moving averages, trend features) directly mirrors the winning approach.

### Abay et al. (2020) — "A machine learning framework for conjunction data message analysis"

This earlier work established a systematic ML framework for CDM analysis.

**Key findings**:
- Demonstrated that **XGBoost with proper feature engineering** (temporal derivatives, covariance ratios, miss distance trends) achieved strong predictive performance on CDM data.
- Identified critical feature groups: **orbital mechanics features** (miss distance, relative velocity, covariance eigenvalues), **temporal features** (time-to-TCA, CDM sequence position), and **metadata features** (object type, radar cross-section).
- Showed that **feature importance analysis** reveals physically meaningful patterns — miss distance and covariance-related features dominate, consistent with orbital mechanics intuition.
- Proposed a pipeline approach: raw CDMs -> feature extraction -> gradient boosted model -> risk classification.

**Relevance to Panacea**: Our feature engineering pipeline follows this framework closely. The identification of temporal derivatives as high-importance features validates our delta/rate-of-change features.

### Metz et al. (2021) — Neural network approaches to CDM analysis

**Key findings**:
- Explored neural network architectures (MLPs, LSTMs) for CDM-based collision risk prediction.
- Found that **gradient boosting remained more effective** on the structured CDM features than neural network approaches.
- Neural networks showed some promise when operating on raw CDM sequences (preserving temporal ordering), but the advantage did not overcome the gap with well-engineered tree-based models.
- Suggested that the relatively small dataset size (13K events) limited what neural networks could learn compared to trees.

**Relevance to Panacea**: Supports our expectation that the PI-TFT will underperform XGBoost. Also motivates why we treat the PI-TFT as a deep learning demonstration rather than expecting it to be the production model.

---

## Competition Landscape Summary

| Rank Tier | Dominant Approach | Key Differentiator |
|-----------|------------------|--------------------|
| Top 5 | XGBoost / LightGBM ensembles | Heavy temporal feature engineering (200-500+ features) |
| Top 20 | Gradient boosting variants | Moderate feature engineering + hyperparameter tuning |
| Middle | Mixed (some neural nets) | Less feature engineering, more default features |
| Lower | Simple baselines, SVMs | Minimal feature processing |

**No neural network approach placed in the top tier.** This is a strong empirical signal about the nature of the problem.

---

## Implications for Panacea

1. **XGBoost is the right tool.** Competition evidence overwhelmingly supports gradient boosting for CDM risk prediction.
2. **Feature engineering is the lever.** The gap between top and middle-tier solutions was feature engineering quality, not model architecture.
3. **Our AUC-PR of 0.978 is credible.** Top competition solutions achieved comparable predictive power with similar approaches.
4. **The PI-TFT faces a documented uphill battle.** Neural networks consistently underperformed trees in this competition, and our results are consistent with that pattern.
