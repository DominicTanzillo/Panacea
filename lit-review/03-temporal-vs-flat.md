<!-- Generated by Claude Code — 2026-02-09 -->

# Do We Need Time Series Modeling for CDMs?

## Overview

Conjunction Data Messages arrive as sequences — multiple CDMs per event, issued at irregular intervals as the Time to Closest Approach (TCA) decreases. This naturally suggests sequential or time series modeling. But does treating CDMs as sequences actually improve predictions compared to "flat" feature engineering on individual CDMs? The evidence is nuanced.

---

## Arguments FOR Sequence Models

### 1. CDMs evolve in physically meaningful ways

As TCA approaches, the orbit determination for both objects improves (more tracking data becomes available). This manifests as:

- **Covariance matrices shrink** over time. The *rate* of shrinkage is informative — rapid convergence suggests good tracking, while slow convergence suggests sparse observations or maneuver uncertainty.
- **Miss distance estimates become more precise** but may also shift. A miss distance that has been steadily decreasing across CDMs is a qualitatively different situation than one that jumped from 500m to 50m in the latest update.
- **Pc estimates converge** (usually). The trajectory of Pc across the CDM sequence contains information beyond the final Pc value alone.

### 2. Temporal patterns carry semantic meaning

Experienced conjunction assessment analysts recognize distinct CDM evolution patterns:

- **Steady convergence**: Miss distance and Pc smoothly converge to final values. Usually low-risk — the orbit is well-determined.
- **Late jump**: Parameters are stable, then suddenly shift in the final CDMs. Often indicates a late orbit determination update or maneuver. Can signal increased risk.
- **Oscillation**: Miss distance and Pc oscillate across CDMs. May indicate competing orbit solutions or unmodeled perturbations. Higher uncertainty.
- **Early alarm, late resolution**: Initial CDMs show high risk, but values relax as more data arrives. Common pattern for eventually-safe events.

A sequence model (RNN, Transformer) can in principle learn to recognize these patterns directly from the data, without the analyst encoding them manually.

### 3. Irregular temporal spacing is informative

The *timing* of CDMs — not just their content — carries information:

- CDMs issued more frequently near TCA indicate increased concern from the screening authority.
- Large gaps in the CDM sequence may indicate periods of poor observability.
- The ratio of `time_to_tca` between consecutive CDMs encodes the monitoring cadence.

Sequence models can naturally incorporate inter-CDM timing as input, whereas flat models require this to be engineered explicitly.

### 4. Theoretical expressiveness

Temporal attention (as in our PI-TFT) can learn to weight recent CDMs more heavily while still referencing earlier observations. This is a principled way to combine the full event history, potentially discovering non-obvious temporal relationships that a human feature engineer might miss.

---

## Arguments AGAINST Sequence Models / Evidence That Flat Works Better

### 1. Kelvins competition winners used flat models

**This is the single strongest piece of evidence.** The competition attracted 73 teams with strong incentives to find the best approach. The winning solutions (Uriot et al. 2022) used:

- Flat gradient boosted tree models (XGBoost, LightGBM)
- Heavy feature engineering on temporal dynamics (deltas between consecutive CDMs, moving averages, rates of change)
- No RNNs, no Transformers, no sequence models

If temporal models had a meaningful advantage, at least one competitive team would have demonstrated it. The absence of sequence models in top solutions is strong evidence that flat feature engineering is sufficient.

### 2. The final CDM contains most of the predictive information

Empirically, the CDM closest to TCA (lowest `time_to_tca`) dominates prediction accuracy. This makes physical sense: the most recent orbit determination is the most accurate, and the reported Pc and miss distance at that point are the best available estimates.

Adding earlier CDMs provides *marginal* additional information. Most of that marginal information can be captured by simple features like "delta miss distance from 2 days ago to now" or "trend in Pc over last 3 CDMs."

### 3. Feature engineering captures temporal dynamics effectively

The temporal information that sequence models would learn can be explicitly engineered:

| Temporal Pattern | Engineered Feature Equivalent |
|-----------------|------------------------------|
| Rate of miss distance change | `delta_miss_distance / delta_time_to_tca` |
| Covariance shrinkage rate | `(cov_latest - cov_previous) / delta_time` |
| Late jump detection | `abs(miss_distance_latest - miss_distance_previous) > threshold` |
| Convergence pattern | Moving average + standard deviation over last N CDMs |
| CDM frequency | `count_of_cdms / time_span` |
| Oscillation | Standard deviation of miss distance across sequence |

These engineered features are interpretable, debuggable, and feed directly into XGBoost's decision-tree splits — a natural fit.

### 4. Small dataset limits sequence model learning

The Kelvins dataset has **~13,000 events** (unique CDM sequences). Sequence models typically need:

- **RNNs**: 50K-100K+ sequences for reliable training
- **Transformers**: 100K+ sequences (attention mechanisms have many parameters)

With 13K events and variable sequence lengths (3-30+ CDMs per event), there is insufficient data for a Transformer to learn temporal representations that outperform hand-crafted features. The model simply cannot observe enough examples of each temporal pattern to generalize reliably.

### 5. Overfitting risk with sequence models on small data

Sequence models have far more parameters than gradient boosted trees:

| Model | Approximate Parameters |
|-------|----------------------|
| XGBoost (500 trees, depth 6) | ~50K effective parameters |
| LSTM (2 layers, 128 hidden) | ~400K parameters |
| PI-TFT (4 layers, 64 dim) | ~500K-1M parameters |

More parameters + less data = higher overfitting risk. Even with regularization (dropout, weight decay, early stopping), the fundamental data-to-parameter ratio works against sequence models here.

---

## Empirical Evidence from Panacea

Our own results confirm the literature:

| Model | Approach | AUC-PR |
|-------|----------|--------|
| Logistic Regression | Flat features, no temporal | ~0.35 |
| **XGBoost** | **Flat features + temporal engineering** | **0.978** |
| PI-TFT | Sequence model with temporal attention | 0.50 |

The XGBoost model with engineered temporal features (deltas, moving averages, rate-of-change features) achieves near-perfect discrimination. The PI-TFT, despite having access to the full temporal sequence via attention, does not come close.

---

## Conclusion

**Temporal modeling is theoretically justified but practically beaten by feature engineering + trees on this dataset size.**

The CDM sequence carries real information, and that information matters for collision risk prediction. However, the question is not whether temporal information exists — it clearly does — but whether a sequence model is the best way to exploit it. On a dataset of 13K events, the answer is no. Hand-crafted temporal features fed to XGBoost capture the relevant dynamics more efficiently and with less overfitting risk.

**The PI-TFT serves as a deep learning demonstration**, illustrating both the potential and the limitations of applying transformers to small structured datasets. It should not be expected to match XGBoost, and its underperformance is not a failure of implementation but a reflection of dataset characteristics. **XGBoost should remain the production model.**

For a future scenario with 100K+ events, streaming CDM data, or multi-modal inputs (combining CDMs with radar observations or optical imagery), the balance could shift toward temporal deep learning. But for the Kelvins dataset as it exists today, trees win.
