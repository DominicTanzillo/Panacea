<!-- Generated by Claude Code — 2026-02-09 -->

# Training Techniques for Improving the PI-TFT

## Overview

While the tabular data literature (04-tabular-dl.md) establishes that trees will likely outperform the PI-TFT on the Kelvins dataset, there are well-studied training techniques that can significantly improve transformer performance on small tabular data. This review covers techniques applicable to the PI-TFT, with practical implementation guidance.

---

## Focal Loss

### Lin et al. (2017) — "Focal Loss for Dense Object Detection"

**Venue**: ICCV 2017 (Best Student Paper)
**Original context**: Object detection, where background examples vastly outnumber objects.

**Core idea**: Standard cross-entropy loss treats all examples equally. In imbalanced datasets, the model spends most of its gradient budget on easy, well-classified majority-class examples. Focal loss *down-weights* easy examples to focus training on hard, informative ones.

**Formula**:
```
FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)
```

Where:
- `p_t` = model's estimated probability for the correct class
- `gamma` (focusing parameter) = controls how much easy examples are down-weighted. `gamma=0` recovers standard cross-entropy. `gamma=2` is the recommended default.
- `alpha_t` (class weight) = balances positive vs. negative classes. For minority class, `alpha=0.75` is recommended.

**How it works**: When `gamma=2`, an example classified with `p_t=0.9` (easy) gets a loss multiplier of `(1-0.9)^2 = 0.01` — essentially ignored. An example classified with `p_t=0.5` (hard) gets a multiplier of `(1-0.5)^2 = 0.25` — 25x more gradient signal than the easy example.

**Relevance to Panacea**: The Kelvins dataset has severe class imbalance (high-risk events are rare). Focal loss with `gamma=2, alpha=0.75` should help the PI-TFT focus on the hard, informative high-risk examples rather than wasting capacity on easy low-risk majority examples.

**Implementation**: Available in `torchvision.ops.sigmoid_focal_loss` or trivially implementable:

```python
def focal_loss(logits, targets, gamma=2.0, alpha=0.75):
    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
    p_t = torch.exp(-bce)
    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
    focal_weight = alpha_t * (1 - p_t) ** gamma
    return (focal_weight * bce).mean()
```

---

## Stochastic Weight Averaging (SWA)

### Izmailov et al. (2018) — "Averaging Weights Leads to Wider Optima and Better Generalization"

**Venue**: UAI 2018

**Core idea**: Instead of using the final checkpoint from training, average the model weights from multiple checkpoints taken during a cyclical or constant learning rate schedule. SWA finds wider, flatter optima in the loss landscape, which generalize better.

**How it works**:
1. Train normally until the model converges.
2. Switch to a cyclical or constant learning rate.
3. At the end of each cycle (or every N epochs), save the weights.
4. Average all saved weight snapshots.
5. Run one final pass with batch normalization statistics update.

**Why it helps**: SGD tends to converge to sharp, narrow minima that memorize training data. SWA's weight averaging effectively moves the solution toward the center of a flat basin in the loss landscape. Flat minima generalize better because small perturbations (from distribution shift between train and test) cause less performance degradation.

**Empirical results**: SWA typically improves generalization by 0.5-1.5% on standard benchmarks. The improvement is often larger on smaller datasets (where sharp minima / overfitting are more common) — directly relevant to our 13K event dataset.

**Implementation**: PyTorch provides built-in support:

```python
from torch.optim.swa_utils import AveragedModel, SWALR

swa_model = AveragedModel(model)
swa_scheduler = SWALR(optimizer, swa_lr=1e-4)

for epoch in range(swa_start, total_epochs):
    train_one_epoch(model)
    swa_model.update_parameters(model)
    swa_scheduler.step()

# Update batch norm statistics
torch.optim.swa_utils.update_bn(train_loader, swa_model)
```

**Relevance to Panacea**: SWA is a low-cost, high-value technique for the PI-TFT. It requires minimal code changes and consistently improves generalization. Recommended as a first-pass improvement to the training pipeline.

---

## Temperature Scaling

### Guo et al. (2017) — "On Calibration of Modern Neural Networks"

**Venue**: ICML 2017

**Core idea**: Modern neural networks are often *overconfident* — they output probabilities close to 0 or 1 even when uncertain. Temperature scaling is a simple post-hoc calibration method that adjusts the model's confidence without changing its discrimination.

**How it works**:
1. Train the model normally.
2. On a held-out validation set, fit a single scalar parameter `T` (temperature) to minimize negative log-likelihood on the validation logits.
3. At inference time: `calibrated_probability = sigmoid(logit / T)`

When `T > 1`, predictions become softer (less confident). When `T < 1`, predictions become more confident. Typical values are `T = 1.2-2.5` for overconfident networks.

**Why it matters**: For conjunction assessment, calibrated probabilities are operationally important. If the model says "70% chance of high risk," operators need that to mean "7 out of 10 events like this are actually high risk," not "this is a coin flip that the model is overconfident about."

**Key property**: Temperature scaling does not change the *ranking* of predictions (AUC-ROC and AUC-PR are unchanged). It only adjusts the *calibration* — how well the predicted probabilities match observed frequencies.

**Implementation**:

```python
import torch
from torch import nn, optim

class TemperatureScaler(nn.Module):
    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1))

    def forward(self, logits):
        return logits / self.temperature

# Fit on validation set
scaler = TemperatureScaler()
optimizer = optim.LBFGS([scaler.temperature], lr=0.01, max_iter=50)

def closure():
    optimizer.zero_grad()
    scaled_logits = scaler(val_logits)
    loss = F.binary_cross_entropy_with_logits(scaled_logits, val_labels)
    loss.backward()
    return loss

optimizer.step(closure)
```

**Relevance to Panacea**: Apply temperature scaling after PI-TFT training to improve calibration. No effect on AUC-PR but improves the operational interpretability of predicted probabilities.

---

## Gradient Accumulation

**Core idea**: When GPU memory is limited, you cannot increase the batch size. Gradient accumulation simulates a larger batch by accumulating gradients over N forward/backward passes before taking an optimizer step.

**Formula**: `effective_batch_size = batch_size * accumulation_steps`

**Why it helps**: Larger effective batch sizes provide:
- More stable gradient estimates (less noise)
- Better utilization of batch normalization statistics
- Smoother training dynamics

For the PI-TFT, where each "sample" is a variable-length CDM sequence that consumes significant memory, gradient accumulation allows using batch sizes that would otherwise be impossible.

**Implementation**:

```python
accumulation_steps = 4  # effective batch = 4 * actual_batch

for i, (inputs, targets) in enumerate(train_loader):
    loss = model(inputs, targets) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**Relevance to Panacea**: If the PI-TFT is memory-constrained (especially with variable-length CDM sequences), gradient accumulation enables larger effective batches without additional memory.

---

## Multi-Task Learning Pitfalls

### Standley et al. (2020) — "Which Tasks Should Be Learned Together in Multi-task Learning?"

**Venue**: ICML 2020

**Core idea**: Multi-task learning (training one model on multiple objectives simultaneously) can help or hurt, depending on task compatibility. When tasks compete for model capacity, **gradient interference** can degrade performance on all tasks.

**Key findings**:
- Tasks with conflicting gradient directions (one task wants to increase a weight, another wants to decrease it) cause **negative transfer**.
- The more dissimilar the tasks, the more likely gradient interference occurs.
- **Mitigation strategies**:
  - Reduce auxiliary task weight (e.g., `total_loss = main_loss + 0.1 * aux_loss` instead of equal weighting)
  - Gradient surgery: project conflicting gradients to remove the interfering component (GradNorm, PCGrad)
  - Task grouping: train compatible tasks together, incompatible tasks separately

**Relevance to Panacea**: The PI-TFT uses a physics-informed loss that combines classification (predict risk) with physics regularization (enforce physical constraints on learned representations). If the physics loss conflicts with the classification gradient, it could be *hurting* performance. Reducing the physics loss weight or applying gradient surgery may help.

---

## Attention Pooling

**Core idea**: Standard approaches take the output of the last position (or a [CLS] token) from a Transformer and feed it to the classification head. **Attention pooling** instead learns a weighted combination of all position outputs.

**How it works**:

```python
class AttentionPooling(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.Linear(hidden_dim, 1)

    def forward(self, sequence_output, mask=None):
        # sequence_output: (batch, seq_len, hidden_dim)
        attn_weights = self.attention(sequence_output).squeeze(-1)  # (batch, seq_len)
        if mask is not None:
            attn_weights = attn_weights.masked_fill(~mask, float('-inf'))
        attn_weights = F.softmax(attn_weights, dim=-1)  # (batch, seq_len)
        pooled = torch.bmm(attn_weights.unsqueeze(1), sequence_output).squeeze(1)
        return pooled  # (batch, hidden_dim)
```

**Why it helps**: For CDM sequences, the most informative CDM might not be the last one. Attention pooling lets the model learn to weight each CDM in the sequence, potentially attending more to CDMs where large parameter changes occurred (orbit determination updates) regardless of their position in the sequence.

**Relevance to Panacea**: Replacing last-position pooling with attention pooling in the PI-TFT could improve its ability to extract relevant temporal information from variable-length CDM sequences.

---

## Optimal Threshold Search

**Core idea**: Binary classifiers default to a threshold of 0.5 (predict positive if `p > 0.5`). For imbalanced datasets, this threshold is almost always suboptimal. Instead, search the precision-recall curve for the threshold that maximizes F1 score (or another target metric).

**How it works**:
1. Generate predicted probabilities on the validation set.
2. Compute precision and recall at many thresholds (e.g., 0.01, 0.02, ..., 0.99).
3. Compute F1 = 2 * precision * recall / (precision + recall) at each threshold.
4. Select the threshold with the highest F1.

**Why it matters**: With class imbalance, the optimal threshold is often much lower than 0.5. For example, if only 5% of events are high-risk, a threshold of 0.15-0.30 might maximize F1 by trading some precision for much better recall.

**Implementation**:

```python
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)
f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
optimal_threshold = thresholds[np.argmax(f1_scores)]
```

**Relevance to Panacea**: Critical for both XGBoost and the PI-TFT. Without threshold optimization, reported F1 may be misleadingly low. This is especially important for the PI-TFT, where poor calibration combined with a default 0.5 threshold could mask better-than-expected discrimination ability.

---

## Summary: Priority Order for PI-TFT Improvements

| Priority | Technique | Expected Impact | Implementation Effort |
|----------|-----------|----------------|----------------------|
| 1 | Optimal Threshold Search | High (directly improves reported F1) | Low (10 lines of code) |
| 2 | Focal Loss | High (addresses class imbalance directly) | Low (swap loss function) |
| 3 | SWA | Medium (better generalization) | Low (PyTorch built-in) |
| 4 | Attention Pooling | Medium (better sequence aggregation) | Medium (architecture change) |
| 5 | Reduce Physics Loss Weight | Medium (if gradient interference exists) | Low (change one hyperparameter) |
| 6 | Temperature Scaling | Low on AUC-PR, high on calibration | Low (post-hoc, no retraining) |
| 7 | Gradient Accumulation | Low-Medium (depends on current batch size) | Low (training loop change) |
