# Generated by Claude Code -- 2026-02-13
"""TLE Staleness Sensitivity Experiment.

Evaluates how model performance degrades as CDM data becomes stale.
Simulates staleness by filtering CDM sequences to only include updates
received at least `cutoff_days` before TCA.
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader

from src.data.cdm_loader import build_events, events_to_flat_features, get_feature_columns
from src.data.sequence_builder import CDMSequenceDataset
from src.evaluation.metrics import evaluate_risk

DEFAULT_CUTOFFS = [0.083, 0.25, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0]
QUICK_CUTOFFS = [0.25, 2.0, 7.0]


def truncate_cdm_dataframe(df: pd.DataFrame, cutoff_days: float) -> pd.DataFrame:
    """Filter CDM rows to only those with time_to_tca >= cutoff_days.

    Simulates data staleness: if cutoff=2.0, the model only sees CDMs
    that arrived 2+ days before closest approach.
    """
    return df[df["time_to_tca"] >= cutoff_days].copy()


def get_ground_truth_labels(df: pd.DataFrame) -> dict:
    """Extract per-event ground truth labels from the FULL (untruncated) dataset.

    Labels come from the final CDM per event (closest to TCA).
    Returns: {event_id: {"risk_label": int, "miss_log": float, "altitude_km": float}}
    """
    labels = {}
    for event_id, group in df.groupby("event_id"):
        group = group.sort_values("time_to_tca", ascending=True)
        final = group.iloc[0]
        risk_label = 1 if final["risk"] > -5 else 0
        miss_log = float(np.log1p(max(final.get("miss_distance", 0.0), 0.0)))
        alt = float(final.get("t_h_apo", 0.0))
        labels[int(event_id)] = {
            "risk_label": risk_label,
            "miss_log": miss_log,
            "altitude_km": alt,
        }
    return labels


def evaluate_baseline_at_cutoff(baseline_model, ground_truth: dict) -> dict:
    """Evaluate baseline model. Baseline uses altitude only, unaffected by staleness."""
    altitudes = np.array([gt["altitude_km"] for gt in ground_truth.values()])
    y_true = np.array([gt["risk_label"] for gt in ground_truth.values()])
    risk_probs, _ = baseline_model.predict(altitudes)
    return evaluate_risk(y_true, risk_probs)


def evaluate_xgboost_at_cutoff(
    xgboost_model,
    truncated_df: pd.DataFrame,
    ground_truth: dict,
    feature_cols: list[str],
) -> dict:
    """Evaluate XGBoost on truncated CDM data."""
    events = build_events(truncated_df, feature_cols)
    if len(events) == 0:
        return {"auc_pr": 0.0, "f1": 0.0, "n_events": 0}

    X, _, _ = events_to_flat_features(events)

    # Pad features if model was trained on augmented data with more columns
    expected_features = xgboost_model.scaler.n_features_in_
    if X.shape[1] < expected_features:
        padding = np.zeros((X.shape[0], expected_features - X.shape[1]), dtype=X.dtype)
        X = np.hstack([X, padding])

    event_ids = [e.event_id for e in events]
    valid_mask = np.array([eid in ground_truth for eid in event_ids])
    X = X[valid_mask]
    y_true = np.array([ground_truth[eid]["risk_label"]
                        for eid in event_ids if eid in ground_truth])

    if len(y_true) == 0 or y_true.sum() == 0:
        return {"auc_pr": 0.0, "f1": 0.0, "n_events": len(y_true)}

    risk_probs = xgboost_model.predict_risk(X)
    metrics = evaluate_risk(y_true, risk_probs)
    metrics["n_events"] = len(y_true)
    return metrics


def evaluate_pitft_at_cutoff(
    model,
    truncated_df: pd.DataFrame,
    ground_truth: dict,
    train_ds: CDMSequenceDataset,
    device: torch.device,
    batch_size: int = 128,
) -> dict:
    """Evaluate PI-TFT on truncated CDM data."""
    test_ds = CDMSequenceDataset(
        truncated_df,
        temporal_cols=train_ds.temporal_cols,
        static_cols=train_ds.static_cols,
    )
    test_ds.set_normalization(train_ds)

    if len(test_ds) == 0:
        return {"auc_pr": 0.0, "f1": 0.0, "n_events": 0}

    loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)

    model.eval()
    all_probs = []

    with torch.no_grad():
        for batch in loader:
            temporal = batch["temporal"].to(device)
            static = batch["static"].to(device)
            tca = batch["time_to_tca"].to(device)
            mask = batch["mask"].to(device)

            risk_logit, _, _ = model(temporal, static, tca, mask)
            probs = torch.sigmoid(risk_logit).cpu().numpy().flatten()
            all_probs.append(probs)

    risk_probs = np.concatenate(all_probs)
    event_ids = [e["event_id"] for e in test_ds.events]
    y_true = np.array([ground_truth.get(eid, {}).get("risk_label", 0) for eid in event_ids])
    valid_mask = np.array([eid in ground_truth for eid in event_ids])

    risk_probs = risk_probs[valid_mask]
    y_true = y_true[valid_mask]

    if len(y_true) == 0 or y_true.sum() == 0:
        return {"auc_pr": 0.0, "f1": 0.0, "n_events": len(y_true)}

    metrics = evaluate_risk(y_true, risk_probs)
    metrics["n_events"] = int(len(y_true))
    return metrics


def run_staleness_experiment(
    baseline_model,
    xgboost_model,
    pitft_model,
    test_df: pd.DataFrame,
    train_ds: CDMSequenceDataset,
    feature_cols: list[str],
    device: torch.device,
    cutoffs: list[float] = None,
) -> dict:
    """Run the full staleness experiment across all cutoffs and models."""
    if cutoffs is None:
        cutoffs = DEFAULT_CUTOFFS

    ground_truth = get_ground_truth_labels(test_df)
    baseline_result = evaluate_baseline_at_cutoff(baseline_model, ground_truth)

    results = {
        "cutoffs": cutoffs,
        "baseline": [],
        "xgboost": [],
        "pitft": [],
    }

    for cutoff in cutoffs:
        print(f"\n--- Staleness cutoff: {cutoff} days ---")
        truncated = truncate_cdm_dataframe(test_df, cutoff)
        n_events = truncated["event_id"].nunique()
        n_rows = len(truncated)
        print(f"  Truncated: {n_rows} rows, {n_events} events remaining")

        # Baseline (constant â€” uses altitude only)
        baseline_entry = {**baseline_result, "cutoff": cutoff}
        results["baseline"].append(baseline_entry)
        print(f"  Baseline AUC-PR: {baseline_result['auc_pr']:.4f}")

        # XGBoost
        if n_events > 0:
            xgb_metrics = evaluate_xgboost_at_cutoff(
                xgboost_model, truncated, ground_truth, feature_cols
            )
        else:
            xgb_metrics = {"auc_pr": 0.0, "f1": 0.0, "n_events": 0}
        xgb_metrics["cutoff"] = cutoff
        results["xgboost"].append(xgb_metrics)
        print(f"  XGBoost AUC-PR: {xgb_metrics['auc_pr']:.4f} "
              f"({xgb_metrics.get('n_events', 0)} events)")

        # PI-TFT
        if n_events > 0 and pitft_model is not None:
            pitft_metrics = evaluate_pitft_at_cutoff(
                pitft_model, truncated, ground_truth, train_ds, device
            )
        else:
            pitft_metrics = {"auc_pr": 0.0, "f1": 0.0, "n_events": 0}
        pitft_metrics["cutoff"] = cutoff
        results["pitft"].append(pitft_metrics)
        print(f"  PI-TFT AUC-PR: {pitft_metrics.get('auc_pr', 0.0):.4f}")

    return results
